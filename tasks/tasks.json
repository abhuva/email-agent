{
  "tasks": [
    {
      "id": 1,
      "title": "Create Configuration Directory Structure",
      "description": "Set up the directory structure for global and account-specific configurations",
      "status": "done",
      "dependencies": [],
      "priority": "high",
      "details": "Create the following directory structure: config/ (for global config.yaml), config/accounts/ (for account-specific YAML files), and ensure proper permissions. Create placeholder files for blacklist.yaml and whitelist.yaml. This task establishes the foundation for the configuration system.",
      "testStrategy": "Verify directory structure exists and is writable. Create test configuration files to ensure proper access.",
      "subtasks": [
        {
          "id": 1,
          "title": "Create base configuration directory structure",
          "description": "Create the root configuration directory and the subdirectory for account-specific configurations.",
          "status": "done",
          "dependencies": [],
          "details": "1) In the project root, create a `config/` directory to hold all configuration files. 2) Inside `config/`, create a subdirectory `accounts/` for account-specific YAML files. 3) If applicable, add these directories to any project scaffolding or setup scripts (e.g., a Makefile target or project bootstrap script) so that they are created automatically on new environments."
        },
        {
          "id": 2,
          "title": "Create global configuration file placeholder",
          "description": "Add a placeholder global config.yaml file in the config directory with minimal valid structure.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Inside `config/`, create an empty but valid YAML file named `config.yaml`. 2) Add a minimal top-level structure, such as comments and example keys, to illustrate intended usage (e.g., `# Global configuration`, `# logging:`, `#   level: info`) without enforcing real values. 3) If using version control, include this file in the repository and consider adding guidance comments about where secrets should NOT be stored (e.g., point to environment variables or separate secrets handling)."
        },
        {
          "id": 3,
          "title": "Create account-specific configuration file placeholders",
          "description": "Set up example account-specific YAML files inside config/accounts/ to demonstrate per-account configuration.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Inside `config/accounts/`, create at least one example YAML file, such as `example-account.yaml`, containing a minimal valid structure (e.g., `# Account-specific configuration`, `# account_id: example`). 2) Optionally add comments showing expected keys (e.g., credentials references, feature toggles) without including real secrets. 3) Decide on and document the naming convention for account files (e.g., `<account-id>.yaml` or `<tenant-name>.yaml`) in comments at the top of the example file."
        },
        {
          "id": 4,
          "title": "Create blacklist and whitelist configuration placeholders",
          "description": "Add placeholder YAML files for blacklist and whitelist configuration and place them appropriately in the config hierarchy.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) In the `config/` directory, create `blacklist.yaml` and `whitelist.yaml` files. 2) Ensure each file contains valid YAML syntax, even if only comments and empty lists (e.g., `blocked_items: []` in `blacklist.yaml` and `allowed_items: []` in `whitelist.yaml`). 3) Add comments explaining the intended semantics and expected data structures (e.g., list of IDs, patterns, or domains) so that future implementation can parse these files consistently."
        },
        {
          "id": 5,
          "title": "Set and document appropriate filesystem permissions for configuration directories and files",
          "description": "Define, apply, and document the required permissions for the config directory tree and its files.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1) Decide on the target runtime user/group that should own the configuration (e.g., application service user). 2) Apply directory permissions so that only the owning user (and optionally group) can read/write the configuration (e.g., `chmod 750 config/ config/accounts/` and `chown <app-user>:<app-group> ...` as appropriate for your OS). 3) Apply file permissions to restrict write access and limit read access to the application and administrators (e.g., `chmod 640 config/*.yaml config/accounts/*.yaml`). 4) If using an infrastructure-as-code or deployment script, codify these permission settings in that script. 5) Add a short section to project documentation (e.g., README or ops docs) describing the directory structure, ownership, and permission requirements so that new environments are configured consistently."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=1 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 1 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 1 - Create Configuration Directory Structure [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement Configuration Loader with Deep Merge Logic",
      "description": "Create the ConfigLoader class that handles loading and merging global and account-specific configurations",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "Create src/config_loader.py with a ConfigLoader class that implements: 1) Loading global config.yaml, 2) Loading account-specific YAML files, 3) Deep merge logic (dictionaries merged, lists replaced, primitives overwritten), 4) Error handling for malformed YAML. The main function should be load_merged_config(account_name: str) -> dict.",
      "testStrategy": "Write unit tests with various configuration scenarios: overriding primitives, merging nested dictionaries, replacing lists. Test error handling with malformed YAML files.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define ConfigLoader interface and file path handling",
          "description": "Create src/config_loader.py with the ConfigLoader class skeleton, constructor, and path resolution for global and account-specific config files.",
          "status": "done",
          "dependencies": [],
          "details": "1) Create src/config_loader.py.\n2) Define a ConfigLoader class with an __init__(self, base_dir: Path | str, global_filename: str = \"config.yaml\", accounts_dirname: str = \"accounts\") that stores resolved pathlib.Path instances for the config root, global config path, and accounts directory.\n3) Add a method _get_global_config_path(self) -> Path that returns the global config file path and does basic existence checks (raise FileNotFoundError with a clear message if missing).\n4) Add a method _get_account_config_path(self, account_name: str) -> Path that resolves the account-specific YAML path (e.g., <base_dir>/<accounts_dirname>/<account_name>.yaml) and can raise FileNotFoundError if the file does not exist (or optionally return None if you want global-only configs to be allowed; choose one behavior and document it).\n5) Add the public method signature load_merged_config(self, account_name: str) -> dict with a docstring describing behavior; leave its implementation to later subtasks."
        },
        {
          "id": 2,
          "title": "Implement YAML loading with error handling",
          "description": "Implement helper methods to load YAML files (global and account-specific) into Python dicts with robust error handling for malformed YAML.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Import yaml (PyYAML) and use yaml.safe_load for reading configuration files to avoid executing arbitrary code.[1][3][7]\n2) Implement a private method _load_yaml_file(self, path: Path) -> dict:\n   - Open the file in text mode with UTF-8 encoding.\n   - Call yaml.safe_load(file_obj).\n   - If the returned value is None, normalize to an empty dict {}.\n   - If the returned value is not a dict (e.g., list or primitive), raise a ValueError explaining that the config root must be a mapping.\n   - Catch yaml.YAMLError and re-raise as a custom ConfigurationError or ValueError with a message including the file path and original exception text.\n3) Implement load_global_config(self) -> dict using _get_global_config_path and _load_yaml_file.\n4) Implement load_account_config(self, account_name: str) -> dict that uses _get_account_config_path and _load_yaml_file; if you chose to allow missing account config in subtask 1, handle FileNotFoundError here by returning {} instead of raising, and document this behavior.\n5) Add unit-test-friendly behavior: avoid printing; rely on raised exceptions for error signaling."
        },
        {
          "id": 3,
          "title": "Implement deep merge utility for configuration dictionaries",
          "description": "Create a pure function or static method that performs deep merge of two configuration dictionaries according to the specified rules.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Define a function deep_merge(base: dict, override: dict) -> dict (either as a @staticmethod on ConfigLoader or a module-level helper) that returns a new merged dict without mutating the input arguments.\n2) Implement merge rules:\n   - When both values for a key are dicts, recursively deep merge them.\n   - When both values are lists, the override list fully replaces the base list (no concatenation).\n   - For all other types (including mismatched types), the override value replaces the base value.\n3) Implement the logic using isinstance checks and recursion; ensure you copy nested dicts/lists (e.g., via dict comprehensions and list slicing) to avoid mutation of arguments.\n4) Ensure that keys present only in base are preserved, keys present only in override are added, and overlapping keys follow the rules above.\n5) Add minimal inline tests or docstring examples to clarify expected behavior, especially for nested structures and list replacement."
        },
        {
          "id": 4,
          "title": "Wire loading and deep merge in load_merged_config",
          "description": "Use the YAML loading helpers and deep merge utility to implement the main load_merged_config(account_name) -> dict method.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "1) In ConfigLoader.load_merged_config(self, account_name: str) -> dict, call load_global_config() to get the base configuration dict.\n2) Call load_account_config(account_name) to get the account-specific configuration dict (which may be empty if missing accounts are allowed).\n3) Pass these to deep_merge, using the global config as base and the account config as override: merged = deep_merge(global_cfg, account_cfg).\n4) Return the merged dict as the final result.\n5) Ensure that any exceptions from loading or malformed YAML propagate clearly to callers; do not silently ignore YAML errors at this layer.\n6) Document in the method docstring that the function applies deep merge with dictionary-merge, list-replace, and primitive-overwrite semantics."
        },
        {
          "id": 5,
          "title": "Add validation, edge-case handling, and tests",
          "description": "Refine robustness of the ConfigLoader and implement tests for loading, deep merge semantics, and malformed YAML handling.",
          "status": "done",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "1) Add validation for the account_name parameter (e.g., disallow path traversal patterns like '../' and strip whitespace) and raise ValueError on invalid names.\n2) Decide and document the behavior for missing global config (likely fail fast with FileNotFoundError or a custom ConfigurationError) and for missing account configs (either allowed -> global-only; or required -> explicit error).\n3) Implement unit tests (e.g., using pytest) that cover:\n   - Successful loading of a simple global config only.\n   - Loading and merging of global + account configs with nested dictionaries, lists, and primitives to verify deep_merge rules.\n   - Behavior when account config is missing (depending on your chosen semantics).\n   - Handling of malformed YAML (e.g., syntax error) verifying that a clear exception is raised.\n   - Type validation at the root level (non-dict root raises error).\n4) Optionally add type hints for all public methods and run a type checker (mypy or similar) to ensure signatures and return types are consistent.\n5) Optionally expose a convenience function at module level, load_merged_config(account_name: str, base_dir: Path | str = DEFAULT_BASE_DIR) -> dict, that internally constructs ConfigLoader and delegates to its method, if this matches the project's expected public API."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=2 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 2 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 2 - Implement Configuration Loader with Deep Merge Logic [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 3,
      "title": "Implement Configuration Schema Validation",
      "description": "Add validation to ensure configurations meet required schema",
      "status": "done",
      "dependencies": [
        2
      ],
      "priority": "high",
      "details": "Extend the ConfigLoader to validate configuration against a schema. Define the schema for required fields, types, and constraints. Implement validation that runs after configuration loading but before returning the merged config. Handle validation errors gracefully, logging issues but allowing processing to continue when possible.",
      "testStrategy": "Create test cases with valid and invalid configurations. Verify validation catches missing required fields, incorrect types, and invalid values.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define configuration schema representation and location",
          "description": "Decide how the configuration schema will be represented (e.g., JSON Schema, custom type/constraint model) and where it will live in the codebase so that required fields, types, and constraints are explicitly defined and easy to maintain.",
          "status": "done",
          "dependencies": [],
          "details": "1) Choose a schema representation appropriate for the current configuration format (e.g., if configs are JSON/YAML, consider using a JSON Schema–like structure or a lightweight custom schema model).\n2) Design a schema data structure that can express: required fields, allowed types, default values (if any), allowed value ranges/enums, and cross-field constraints if needed.\n3) Decide on schema ownership and loading strategy: hard-coded in code (e.g., a constant), loaded from a separate schema file, or composed from multiple module-level schemas.\n4) Implement the schema definition for the existing configuration surface (all currently supported config keys) using the chosen representation.\n5) Add basic unit tests that verify the schema object/schema file contains expected keys and constraints and can be loaded/instantiated without errors."
        },
        {
          "id": 2,
          "title": "Implement core schema validation engine",
          "description": "Create a reusable validation module that can take a loaded config object and the schema definition and return structured validation results (success, warnings, and errors).",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Introduce a dedicated validator component (e.g., ConfigSchemaValidator) that accepts (schema, config) and returns a result object containing: `isValid`, `errors[]`, `warnings[]`, and possibly normalized config data.\n2) Implement per-field validation: existence of required fields, type checks, default value filling (if supported), and basic constraints (min/max, enum, regex, etc.) according to the schema model.\n3) Implement schema-level validation hooks for cross-field rules if the schema representation supports them (e.g., callbacks or expression-based rules evaluated on the entire config object).\n4) Make validation errors and warnings structured (e.g., each item includes path/key, error code, human-readable message, and raw value) so callers can log and act on them.\n5) Add unit tests for the validator module that cover: missing required fields, wrong types, invalid constraint values, valid configs, and mixed warnings/errors."
        },
        {
          "id": 3,
          "title": "Integrate schema validation into ConfigLoader workflow",
          "description": "Extend the existing ConfigLoader to invoke the schema validation engine after loading and merging configuration sources but before returning the final configuration object.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1) Identify the point in ConfigLoader where all configuration sources have been loaded and merged into a single in-memory config object.\n2) Inject or construct the ConfigSchemaValidator and pass the merged config plus the schema defined in subtask 1.\n3) Update the ConfigLoader public API to propagate validation results appropriately, e.g., by returning a tuple (config, validationResult) or by exposing a separate method to retrieve validation diagnostics.\n4) Ensure that the validation is always executed in the normal code path (unless explicitly disabled by configuration/flags, if desired).\n5) Add integration tests that load realistic configs through ConfigLoader and assert that validation is invoked and that results match expectations."
        },
        {
          "id": 4,
          "title": "Handle validation errors and warnings with graceful degradation",
          "description": "Define and implement policies for how ConfigLoader behaves on validation failures, including logging behavior, when to abort vs. continue, and how to surface issues to callers without crashing unnecessarily.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "1) Design severity levels and behavior: decide which validation issues are fatal (abort loading and throw/return an error) and which are non-fatal (log and continue), possibly driven by error codes or configuration flags (e.g., `strictValidation` mode).\n2) Implement logging for all validation issues using the project’s logging framework, including configuration key path, severity, and message; ensure logs are easily searchable.\n3) Implement control flow in ConfigLoader to:\n   - Abort and fail fast when fatal validation errors occur (e.g., missing critical required fields) and return a clear error to the caller.\n   - Allow processing to continue when only non-critical issues are found, while still returning/logging the validationResult for visibility.\n4) Consider adding hooks/callbacks so higher-level components can react to validation failures (e.g., show warnings in UI, metrics, or alerts).\n5) Extend integration tests to cover different error-handling scenarios (fatal errors, non-fatal warnings, strict vs. lenient behavior) and assert both logs and return values."
        },
        {
          "id": 5,
          "title": "Document and test configuration schema validation end-to-end",
          "description": "Add documentation and comprehensive tests that explain how schema validation works, how to extend the schema when adding new configuration options, and verify the entire flow from loading to validation and error handling.",
          "status": "done",
          "dependencies": [
            3,
            4
          ],
          "details": "1) Write developer documentation (e.g., in README or internal docs) describing: the schema format, how to add/modify config keys in the schema, how validation is triggered in ConfigLoader, and how to interpret validation errors/warnings.\n2) Document configuration options that influence validation behavior (e.g., strict mode, toggles to disable validation, log verbosity) if any were introduced.\n3) Add end-to-end tests that simulate real-world configuration files (valid, partially valid, and invalid) and assert on: final returned config, validationResult contents, and logged messages.\n4) If applicable, add regression tests for any previously known misconfigurations to ensure they are now caught by validation.\n5) Ensure CI runs the new tests and that failing validation behaviors are clearly visible in test output so future schema changes are safe and intentional."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=3 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 3 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 3 - Implement Configuration Schema Validation [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 4,
      "title": "Create EmailContext Data Class",
      "description": "Implement the EmailContext data class to track email state through the processing pipeline",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "high",
      "details": "Create a data class in src/models.py that contains all email metadata and state: uid, sender, subject, raw_html, raw_text, parsed_body, is_html_fallback, llm_score, llm_tags, whitelist_boost, whitelist_tags, and result_action. This class will be passed through the processing pipeline and maintain the state of each email.",
      "testStrategy": "Create unit tests to verify proper initialization, attribute access, and state transitions of the EmailContext class.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define EmailContext dataclass structure in src/models.py",
          "description": "Create the EmailContext dataclass with all required fields and type hints inside src/models.py.",
          "status": "done",
          "dependencies": [],
          "details": "1. Open/create src/models.py.\n2. Import dataclass utilities: `from dataclasses import dataclass, field` and typing primitives like `from typing import Optional, List` (or other containers as appropriate).\n3. Declare `@dataclass` and define `class EmailContext:`.\n4. Add fields with explicit type hints for all required attributes: `uid`, `sender`, `subject`, `raw_html`, `raw_text`, `parsed_body`, `is_html_fallback`, `llm_score`, `llm_tags`, `whitelist_boost`, `whitelist_tags`, and `result_action`.\n5. Use appropriate types, for example: `uid: str`, `sender: str`, `subject: str`, `raw_html: Optional[str]`, `raw_text: Optional[str]`, `parsed_body: Optional[str]` (or a richer type if parser returns a structure), `is_html_fallback: bool`, `llm_score: Optional[float]`, `llm_tags: List[str]`, `whitelist_boost: float`, `whitelist_tags: List[str]`, `result_action: Optional[str]`.\n6. Ensure the file exports EmailContext (e.g., by listing it in `__all__` if that pattern is used in the project)."
        },
        {
          "id": 2,
          "title": "Configure sensible defaults and mutability for EmailContext fields",
          "description": "Add default values and factories so that EmailContext instances can be created safely and predictably throughout the pipeline.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1. Decide which fields must be required vs optional at construction; remove defaults from required fields (e.g., `uid`, `sender`, `subject`) so Python enforces them.\n2. For optional or pipeline-populated fields, set defaults:\n   - Use `Optional[...] = None` for values not known at instantiation (e.g., `parsed_body`, `llm_score`, `result_action`).\n   - Use `field(default_factory=list)` for list fields like `llm_tags` and `whitelist_tags` to avoid shared mutable defaults.\n   - Provide reasonable numeric defaults (e.g., `whitelist_boost: float = 0.0`, `is_html_fallback: bool = False`).\n3. Decide whether the dataclass should be mutable; if the pipeline updates the same instance step by step, keep `@dataclass` without `frozen=True`.\n4. Optionally configure `repr`/`eq` behavior (e.g., accept defaults, or exclude large fields like `raw_html` from repr using `field(repr=False)` if necessary for logging readability)."
        },
        {
          "id": 3,
          "title": "Add documentation and helper methods to EmailContext",
          "description": "Document the purpose of EmailContext and add lightweight helper methods that encapsulate common state transitions or checks.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1. Add a class docstring explaining that EmailContext tracks an email’s metadata and processing state through the pipeline, and briefly describe each field’s role.\n2. Add small convenience methods where useful, for example:\n   - `def add_llm_tag(self, tag: str) -> None:` to append to `llm_tags` while preventing duplicates if desired.\n   - `def add_whitelist_tag(self, tag: str, boost: float = 0.0) -> None:` to register a whitelist tag and optionally adjust `whitelist_boost`.\n   - `def is_scored(self) -> bool:` returning `self.llm_score is not None`.\n   - `def has_result(self) -> bool:` returning `self.result_action is not None`.\n3. Keep methods minimal and side-effect clear, since this class is primarily a state container used across multiple pipeline stages.\n4. If your codebase uses type checking (mypy/pyright), ensure method signatures match existing conventions (e.g., return types annotated, no unused parameters)."
        },
        {
          "id": 4,
          "title": "Integrate EmailContext into the processing pipeline",
          "description": "Update pipeline components to construct, pass, and mutate EmailContext instances instead of using ad-hoc dictionaries or parameter sets.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "1. Identify all existing pipeline entry points where email data is first available (e.g., IMAP fetchers, webhook handlers) and replace current email-structure representations with `EmailContext` construction.\n2. Ensure all necessary fields are populated at creation time: `uid`, `sender`, `subject`, `raw_html`/`raw_text`, and any other immediately known values.\n3. For each pipeline stage (parsing, scoring, whitelisting, action selection, etc.), update signatures to accept and return `EmailContext` instead of individual parameters or generic containers.\n4. Within each stage, write to the corresponding fields on the `EmailContext` instance (e.g., parser sets `parsed_body` and `is_html_fallback`, LLM step sets `llm_score` and `llm_tags`, whitelist step sets `whitelist_tags` and adjusts `whitelist_boost`, final decision step sets `result_action`).\n5. Ensure that logging and debugging statements use `EmailContext`’s repr or selected attributes, avoiding printing large raw content if that is a concern."
        },
        {
          "id": 5,
          "title": "Add tests for EmailContext behavior and pipeline usage",
          "description": "Create or extend tests to validate EmailContext structure, defaults, and correct state transitions across pipeline steps.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "1. In the test suite (e.g., tests/test_models.py and relevant pipeline test files), add tests that instantiate `EmailContext` with minimal required fields and assert defaults for optional fields (None, empty lists, numeric defaults, etc.).\n2. Test helper methods added to EmailContext (e.g., `add_llm_tag`, `add_whitelist_tag`, `is_scored`, `has_result`) to confirm they behave as expected and mutate state correctly.\n3. Write integration-style tests for key pipeline flows that:\n   - Construct an initial `EmailContext`.\n   - Pass it through the main pipeline stages.\n   - Assert that fields such as `parsed_body`, `llm_score`, `llm_tags`, `whitelist_boost`, `whitelist_tags`, and `result_action` are set/updated as expected.\n4. If type checking or linting is part of CI, run those tools to ensure EmailContext and its usage conform to project standards.\n5. Adjust tests and implementation if any missing or mis-typed fields are discovered during testing."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=4 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 4 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 4 - Create EmailContext Data Class [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement Content Parser with HTML to Markdown Conversion",
      "description": "Create the content parser module to convert HTML emails to Markdown",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Create src/content_parser.py that integrates the html2text library. Implement parse_html_content(html_body: str, plain_text_body: str) -> (parsed_content: str, is_fallback: bool) function that converts HTML to Markdown, falls back to plain text on failure, and enforces the 20,000 character limit. Add proper error handling and logging.",
      "testStrategy": "Test with various HTML inputs including complex formatting, malformed HTML, and edge cases. Verify character limit enforcement and fallback mechanism.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up content_parser module and html2text dependency",
          "description": "Create the content parser module file and ensure the html2text library is available and imported correctly.",
          "status": "done",
          "dependencies": [],
          "details": "1) Add html2text to the project dependencies (e.g., pyproject.toml/requirements.txt) so it is installed with the application.[1][4] 2) Create src/content_parser.py if it does not exist. 3) In src/content_parser.py, import html2text (either using the functional API html2text.html2text or the HTML2Text class, depending on configuration needs).[1][6][7] 4) Define the public function signature parse_html_content(html_body: str, plain_text_body: str) -> tuple[str, bool] (or -> (str, bool) for older Python) as a stub that currently just returns the plain_text_body and a fallback flag placeholder."
        },
        {
          "id": 2,
          "title": "Implement HTML to Markdown conversion using html2text",
          "description": "Use html2text to convert HTML email bodies to Markdown, encapsulated in a helper inside the content_parser module.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Inside src/content_parser.py, create a private helper function _html_to_markdown(html_body: str) -> str. 2) Instantiate html2text.HTML2Text() so you can configure behavior (e.g., body_width = 0 to avoid wrapping, configure link handling, etc.).[1][7][5][8] 3) Use h.handle(html_body) on the HTML2Text instance to produce the Markdown string.[1][6][7] 4) Ensure the helper returns the converted text as a Python str. 5) Add basic normalization such as stripping leading/trailing whitespace if desired, but do not yet implement limit enforcement or fallback logic in this helper."
        },
        {
          "id": 3,
          "title": "Add error handling and fallback to plain text",
          "description": "Ensure parse_html_content uses the HTML conversion with robust error handling and falls back to the plain text body when needed.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1) In parse_html_content, wrap the call to _html_to_markdown in a try/except block catching broad exceptions (e.g., Exception) to handle unexpected errors from html2text.[1][6] 2) If html_body is missing, empty, or only whitespace, skip HTML conversion and directly use plain_text_body as the output content, setting is_fallback = True. 3) If _html_to_markdown raises an exception, log the error (to be implemented in the logging subtask) and use plain_text_body as the output content with is_fallback = True. 4) If HTML conversion succeeds and produces a non-empty string, set parsed_content to that value and is_fallback = False. 5) If HTML conversion technically succeeds but returns an empty or whitespace-only string, treat this as a failed conversion and fall back to plain_text_body with is_fallback = True."
        },
        {
          "id": 4,
          "title": "Enforce 20,000-character limit on parsed content",
          "description": "Apply a hard length limit of 20,000 characters to the returned content, after conversion and fallback handling.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "1) After determining parsed_content (whether from HTML or plain_text fallback) and before returning from parse_html_content, check len(parsed_content). 2) If the string exceeds 20_000 characters, truncate it to parsed_content[:20_000]. 3) Decide and document that truncation does not change the is_fallback flag: if content came from HTML and was truncated, is_fallback remains False; if it came from plain_text, is_fallback remains True. 4) Optionally, avoid expensive length recomputations by only slicing when len(parsed_content) > 20_000. 5) Keep the limit logic isolated inside parse_html_content so callers always receive content that respects the size constraint."
        },
        {
          "id": 5,
          "title": "Integrate logging and add basic tests for content_parser",
          "description": "Add structured logging for errors and key decision branches, and create tests to validate conversion, fallback, and limit behavior.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "1) Use the standard logging module (or the project’s logging wrapper) in src/content_parser.py, creating a module-level logger via logging.getLogger(__name__). 2) Log at debug/info level when HTML conversion is attempted and whether it is used or skipped (e.g., missing HTML, empty HTML). 3) Log at warning/error level when HTML conversion fails and the parser falls back to plain_text_body, including exception details. 4) Log when content is truncated due to exceeding 20,000 characters, ideally including original length and truncated length. 5) Create unit tests (e.g., in tests/test_content_parser.py) that cover: a) successful HTML-to-Markdown conversion with is_fallback = False, b) missing/empty HTML with fallback to plain text and is_fallback = True, c) forced exception in _html_to_markdown to verify error logging and fallback, d) enforcement of the 20,000-character limit for both HTML-derived and plain-text-derived content."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=5 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 5 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 5 - Implement Content Parser with HTML to Markdown Conversion [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 6,
      "title": "Implement Rules Engine - Blacklist Rules",
      "description": "Create the blacklist component of the rules engine",
      "status": "done",
      "dependencies": [
        4
      ],
      "priority": "high",
      "details": "Create src/rules.py with functions to load and process blacklist rules. Implement check_blacklist(email_obj, rules) -> ActionEnum that returns DROP, RECORD, or PASS based on matching rules. Support sender, subject, and domain triggers as specified in the PRD. Include proper error handling for malformed rules.",
      "testStrategy": "Create test cases with various blacklist rules and email scenarios. Verify correct actions are returned for different trigger types and values.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define blacklist rule data structures and ActionEnum integration",
          "description": "Introduce core types for blacklist rules and ensure compatibility with the existing ActionEnum used by the rules engine.",
          "status": "done",
          "dependencies": [],
          "details": "1) Inspect the existing codebase to locate the ActionEnum definition (or create it if this task includes that), confirming the available actions include DROP, RECORD, and PASS. 2) In src/rules.py, define a clear internal representation for blacklist rules, e.g., a dataclass or typed dict such as `BlacklistRule` with fields like `trigger_type` (\"sender\" | \"subject\" | \"domain\"), `pattern` (string or compiled regex), and `action` (ActionEnum). 3) Decide whether wildcard/regex support is required based on the PRD; if yes, add fields to store the raw pattern and a compiled matcher. 4) Add type hints for all new structures and functions (using `typing` and possibly `dataclasses`) to guide later implementation and testing. 5) Document the expected rule schema in comments or docstrings so later subtasks can reference it (e.g., example rule objects and which fields are mandatory)."
        },
        {
          "id": 2,
          "title": "Implement rule loading and validation with error handling",
          "description": "Create functions in src/rules.py to load blacklist rules from the configured source and validate them, raising or logging appropriate errors for malformed rules.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Based on the PRD, determine the source/format for blacklist rules (e.g., JSON file, YAML, DB rows, or in-memory config) and define a `load_blacklist_rules(source_config) -> list[BlacklistRule]` function that reads and parses them into the data structure defined in Subtask 1. 2) Implement a dedicated validation function like `validate_blacklist_rule(raw_rule) -> BlacklistRule` that checks required fields (trigger type, pattern, action), validates trigger type against supported values (sender, subject, domain), and maps action strings to ActionEnum, failing fast on unknown actions. 3) Add robust error handling: for malformed rules, either skip them and log a structured warning or raise a custom exception (e.g., `InvalidRuleError`) according to project conventions; ensure errors clearly indicate which rule failed and why. 4) If regex or wildcard patterns are used, compile them in this step and catch compilation errors, treating them as malformed rules. 5) Add unit tests or at least stub tests for valid, partially valid, and malformed rule inputs to confirm correct behavior and error handling."
        },
        {
          "id": 3,
          "title": "Implement rule matching helpers for sender, subject, and domain triggers",
          "description": "Create reusable helper functions that check whether a given email_obj matches a single blacklist rule for each supported trigger type.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) Define a small interface for `email_obj` expectations in docstrings or types (e.g., it must expose `sender`, `subject`, and `domain` properties or fields), aligning with how emails are represented elsewhere in the project. 2) Implement helper functions such as `match_sender_rule(email_obj, rule) -> bool`, `match_subject_rule(email_obj, rule) -> bool`, and `match_domain_rule(email_obj, rule) -> bool` (or a generic `rule_matches_email(email_obj, rule) -> bool` that internally dispatches based on `rule.trigger_type`). 3) For each trigger type, implement the matching logic according to the PRD: e.g., exact match, case-insensitive match, substring/contains, wildcard, or regex as specified. 4) Ensure matching functions gracefully handle missing or empty fields on `email_obj` (e.g., no subject) without raising unexpected exceptions, returning False instead and letting higher layers decide on default behavior. 5) Add targeted tests for each helper to cover positive matches, negative matches, case sensitivity rules, and edge cases like empty strings or None values."
        },
        {
          "id": 4,
          "title": "Implement check_blacklist(email_obj, rules) -> ActionEnum",
          "description": "Create the main blacklist evaluation function that iterates over rules, applies matching logic, and returns the appropriate ActionEnum value.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "1) In src/rules.py, implement `check_blacklist(email_obj, rules: list[BlacklistRule]) -> ActionEnum`. 2) For each rule in `rules`, use the helper matching functions from Subtask 3 to determine if the rule applies to the given email_obj. 3) Define and implement the evaluation strategy per the PRD: e.g., first-match-wins, priority based on rule order, or rule-specific precedence (such as DROP overriding RECORD, which overrides PASS). If the PRD defines explicit priority rules, encode them clearly in the logic; otherwise adopt a consistent, documented strategy. 4) Ensure that any unexpected issues (e.g., a rule missing a required field) are handled defensively—either by catching exceptions from malformed rules and logging them or by skipping those rules, without causing the entire evaluation to fail. 5) If no blacklist rules match, return the default ActionEnum (likely PASS) as specified in the PRD; document this default behavior in the function’s docstring and tests."
        },
        {
          "id": 5,
          "title": "Integrate, test, and document the blacklist rules component",
          "description": "Wire the blacklist component into the existing rules engine flow, add comprehensive tests, and document usage and error cases.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "1) Integrate `load_blacklist_rules` and `check_blacklist` into the broader rules engine or email processing pipeline, ensuring they are called at the correct stage in message handling (e.g., pre-processing before other rules, as specified in the PRD). 2) Create unit tests and, if applicable, integration tests that cover end-to-end scenarios: emails that should be DROPed, RECORDED, or PASSED through, including combinations of sender, subject, and domain rules, and cases where multiple rules match with different actions. 3) Add tests for malformed rules at load time and confirm that the system behavior matches the expected error-handling strategy (e.g., skipped with logs vs. raising). 4) Document the blacklist rules behavior in code comments or project documentation: expected rule schema, supported trigger types, matching semantics (exact/contains/regex), precedence rules, and default action when no match occurs. 5) If any configuration flags or environment variables are used to control blacklist behavior (e.g., enabling/disabling blacklist, rule file paths), ensure they are documented and validated, and add tests for configuration-driven behavior where possible."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=6 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 6 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 6 - Implement Rules Engine - Blacklist Rules [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 7,
      "title": "Implement Rules Engine - Whitelist Rules",
      "description": "Create the whitelist component of the rules engine",
      "status": "done",
      "dependencies": [
        6
      ],
      "priority": "high",
      "details": "Extend src/rules.py to handle whitelist rules. Implement apply_whitelist(email_obj, rules, current_score) -> (new_score, tags_list) that applies score boosts and adds tags based on matching rules. Support the same trigger types as blacklist rules. Ensure proper validation of score_boost values and tags.",
      "testStrategy": "Test with various whitelist rules and verify score adjustments and tag additions. Test edge cases like extremely high score boosts and duplicate tags.",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze existing rules engine and blacklist implementation",
          "description": "Review the current rules engine structure in src/rules.py (and any related modules/tests) to understand how blacklist rules are represented, validated, and applied. Document the rule schema, supported trigger types, and the expected return format (score, tags list) to ensure whitelist behavior is consistent.",
          "status": "done",
          "dependencies": [],
          "details": "• Open src/rules.py and locate the existing blacklist handling function(s) and any shared helpers (e.g., apply_blacklist, trigger evaluators, validation utilities).\n• Identify how rules are structured (Python dicts, classes, or configs), what fields a rule contains (e.g., trigger_type, pattern, score_impact, tags), and how trigger types are dispatched.\n• Confirm the contract for rule application functions: input parameters (email_obj, rules, current_score) and output format (new_score, tags_list) as used elsewhere in the codebase.\n• Search for tests or usages of blacklist rules to see real examples of rules and expected behavior; note how errors and invalid rules are currently handled.\n• Summarize findings (schema, trigger types, helper functions, validation patterns) in comments or a short internal doc to use as reference when implementing whitelist logic."
        },
        {
          "id": 2,
          "title": "Define whitelist rule schema, validation, and score_boost/tag constraints",
          "description": "Specify and implement the data schema and validation rules for whitelist entries, ensuring that score_boost and tags fields are well-defined, type-checked, and consistent with or complementary to existing blacklist rules.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "• Based on the blacklist schema, define the whitelist rule structure (e.g., {\"trigger_type\": str, \"pattern\": str, \"score_boost\": int|float, \"tags\": [str], ...}).\n• Decide and document acceptable ranges/types for score_boost (e.g., numeric only, min/max bounds if appropriate) and ensure it represents a positive adjustment (or at least non-negative) distinct from blacklist penalties.\n• Define constraints for tags: must be a list/iterable of non-empty strings; optionally normalize (e.g., lowercasing, trimming whitespace) if that is consistent with existing tagging logic.\n• Implement a whitelist-specific validation function or extend existing rule validation utilities to check: required fields present, correct types, allowed trigger_type values (matching blacklist-supported triggers), and valid score_boost/tags.\n• Ensure validation errors are handled in a consistent way with the rest of the rules engine (e.g., raising a specific exception type or skipping invalid rules with logging)."
        },
        {
          "id": 3,
          "title": "Implement trigger evaluation and matching for whitelist rules",
          "description": "Implement or reuse trigger evaluation logic for whitelist rules so they support the same trigger types as blacklist rules, ensuring that each rule can correctly determine whether it matches a given email_obj.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "• From the analysis in subtask 1, list all supported trigger types for blacklist (e.g., sender, subject_contains, header_matches, body_regex, domain, etc.).\n• Identify any existing shared trigger evaluation helpers; if they are generic, confirm they can be reused directly for whitelisting. If they are blacklist-specific, refactor them into neutral, reusable functions without changing external behavior.\n• Implement trigger evaluation for each supported trigger_type, ensuring that the same semantics used for blacklist (e.g., case sensitivity, regex flags, partial vs exact match) are preserved.\n• Write a single entry point (e.g., match_whitelist_rule(email_obj, rule) -> bool) or similar abstraction that apply_whitelist can call to determine if an individual whitelist rule matches.\n• Add inline documentation/comments describing each trigger type’s behavior and any assumptions (e.g., what parts of email_obj are required, how missing attributes are handled)."
        },
        {
          "id": 4,
          "title": "Implement apply_whitelist(email_obj, rules, current_score)",
          "description": "Add the apply_whitelist function in src/rules.py that iterates over whitelist rules, applies score boosts, and accumulates tags when rules match, then returns the updated score and tag list while enforcing validation.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "• Define the function signature apply_whitelist(email_obj, rules, current_score) -> (new_score, tags_list) as specified.\n• At the start of the function, validate the rules collection: run each rule through the whitelist validation logic; decide whether to raise on invalid rules or skip them with logging, mirroring blacklist behavior.\n• Initialize new_score = current_score and tags_list as an empty list (or existing tags if the engine expects that pattern; verify from blacklist implementation).\n• For each valid whitelist rule:\n  – Use the trigger matching abstraction from subtask 3 to check if the rule matches email_obj.\n  – If matched, apply the rule’s score_boost to new_score (e.g., new_score += rule[\"score_boost\"]).\n  – Merge rule tags into tags_list, ensuring type correctness and optionally avoiding duplicates if that matches existing tag behavior.\n• Decide on and document the rule application strategy (e.g., apply all matching rules vs stop after first match) to mirror blacklist logic or project requirements; implement accordingly.\n• Ensure the function returns (new_score, tags_list) with types and structure consistent with other rule application functions in the module."
        },
        {
          "id": 5,
          "title": "Integrate whitelist rules into engine workflow and add tests",
          "description": "Wire apply_whitelist into the existing rules engine flow (wherever whitelist behavior is expected) and create unit tests to verify correct handling of score_boost, tags, trigger types, and validation edge cases.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "• Identify the components or pipeline stages that should call apply_whitelist (e.g., main scoring function, email classification pipeline). Integrate the function in the appropriate order relative to blacklist application (e.g., apply_whitelist before/after blacklist according to project logic).\n• Ensure configuration/loading logic for rules distinguishes and correctly passes whitelist rules into apply_whitelist.\n• Implement unit tests covering:\n  – Single matching whitelist rule (score increased, tags added correctly).\n  – Multiple matching rules and their cumulative effects on score and tags.\n  – Non-matching rules (no score or tag changes).\n  – Validation failures (invalid score_boost values, malformed tags, unsupported trigger_type) and expected behavior (exception or skip with logging).\n  – Behavior consistency across all supported trigger types (each trigger type has at least one passing and failing test case).\n• If an automated test suite exists, run it and adjust implementation to ensure no regressions in blacklist or other rule engine behavior.\n• Add or update inline documentation/docstrings to describe how whitelist rules should be configured and how apply_whitelist interacts with the overall scoring process."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=7 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 7 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 7 - Implement Rules Engine - Whitelist Rules [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 8,
      "title": "Create Account Processor Class",
      "description": "Implement the AccountProcessor class for isolated per-account processing",
      "status": "done",
      "dependencies": [
        5,
        7
      ],
      "priority": "high",
      "details": "Create src/account_processor.py with an AccountProcessor class that handles the processing pipeline for a single account. Implement setup(), run(), and teardown() methods. Include IMAP connection management and the core processing pipeline: blacklist check, content parsing, LLM processing, whitelist modifiers, and note generation. Ensure complete state isolation.",
      "testStrategy": "Create unit tests with mock IMAP connections. Test the full pipeline with various email scenarios. Verify state isolation by processing multiple accounts sequentially.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define AccountProcessor class skeleton and state isolation",
          "description": "Create src/account_processor.py with the AccountProcessor class, its constructor, and core attributes ensuring per-account state isolation.",
          "status": "done",
          "dependencies": [],
          "details": "1. Create the file src/account_processor.py.\n2. Define a class `AccountProcessor` following your project’s naming and style conventions.\n3. Design `__init__(self, account_config, imap_client_factory, llm_client, blacklist_service, whitelist_service, note_generator, parser, logger)` (adapt names to existing abstractions) so that all dependencies for processing a single account are passed in or derived from immutable configuration rather than using globals or shared mutable state.\n4. Store only account-specific state on `self` (e.g., `self.account_id`, `self.config`, `self.logger`, `self._imap_conn`, `self._processing_context`), avoiding class variables and shared singletons.\n5. Document the class docstring explaining its responsibility: isolated processing pipeline for a single account, including IMAP connection management and pipeline orchestration.\n6. Ensure that any reusable utilities are injected or imported as stateless helpers, not as shared mutable objects, to preserve isolation between AccountProcessor instances."
        },
        {
          "id": 2,
          "title": "Implement setup() with IMAP connection management and initialization",
          "description": "Implement the setup phase to prepare all resources required for processing a single account, with a focus on IMAP connection lifecycle.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1. Add a `setup(self)` instance method to AccountProcessor.\n2. Inside `setup`, establish the IMAP connection using the injected `imap_client_factory` or existing IMAP wrapper, using credentials and server details from `self.config`.\n3. Store the resulting connection object on `self._imap_conn`, and ensure it is not shared with other accounts (one connection per AccountProcessor instance).\n4. Perform any account-specific initialization required by downstream components (e.g., load account-specific blacklist/whitelist data, prefetch folders, initialize a per-run processing context dict or object on `self._processing_context`).\n5. Add robust error handling: if IMAP connection or initialization fails, log the error through `self.logger` and either raise a well-defined exception or set an internal failure state that `run()` can inspect.\n6. Ensure `setup` is idempotent or clearly documented as single-use; guard against reusing a closed or already-open connection if that can occur in your environment."
        },
        {
          "id": 3,
          "title": "Design and implement the core processing pipeline in run()",
          "description": "Implement run() to orchestrate the per-account processing pipeline: fetch items, apply blacklist check, parse content, call LLM, apply whitelist modifiers, and produce notes.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1. Add a `run(self)` method that assumes `setup()` has succeeded (optionally validate internal state, e.g., `self._imap_conn` is not None, and fail fast if not).\n2. Decide on the processing unit (e.g., individual emails/messages) and obtain them from the IMAP connection by calling existing utility functions or methods on `self._imap_conn`.\n3. For each item:\n   - Perform **blacklist check** by consulting the injected `blacklist_service` or equivalent; skip or mark messages failing the blacklist criteria, and log the action.\n   - Perform **content parsing** using the injected `parser` to extract normalized fields (sender, subject, body, attachments, metadata) into a structured object stored in a local variable or attached to the processing context.\n   - Perform **LLM processing** by calling the injected `llm_client` with the parsed content, following existing prompt/response patterns; capture outputs such as classifications, summaries, or decisions.\n   - Apply **whitelist modifiers** using `whitelist_service` or equivalent, adjusting the LLM or parsed results according to account-specific whitelist rules (e.g., promoting/allowing certain senders or patterns).\n   - Perform **note generation** by calling the injected `note_generator` with the combined parsed content, LLM result, and whitelist-modified data to produce the final note object or text.\n4. Collect results (e.g., list of generated notes and any metadata) in a local list or a clearly scoped attribute of `self._processing_context`, not in any shared/global structure.\n5. Add logging at key steps (start/end of run, per-stage errors, counts of processed/filtered messages).\n6. Ensure exceptions in processing a single item are caught and logged without breaking the entire account run, unless failures are unrecoverable; define and use internal helper methods (e.g., `_process_message(msg)`) if this improves clarity while keeping state encapsulated."
        },
        {
          "id": 4,
          "title": "Implement teardown() and resource cleanup guaranteeing isolation",
          "description": "Implement teardown() to reliably release IMAP connections and any other per-account resources, preserving clean state boundaries.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "1. Add a `teardown(self)` method responsible for cleaning up all resources allocated in `setup()` and `run()`.\n2. If `self._imap_conn` is open, close or logout via the appropriate IMAP API calls, handling and logging any exceptions during close without raising them further unless required by your error-handling design.\n3. Clear or reset any per-run state stored in `self._processing_context` and other transient attributes (e.g., temporary caches, message buffers) to avoid accidental reuse across future runs of the same object (or document if objects are single-use).\n4. Ensure that no global or shared resources are modified during teardown, only this instance’s resources.\n5. Consider implementing the context manager protocol (`__enter__` / `__exit__`) to call `setup()` and `teardown()` automatically if that matches project patterns, but keep the primary public API as explicit `setup()`, `run()`, `teardown()`.\n6. Add tests (or at least log checks) to verify that after `teardown()`, the IMAP connection is closed and internal state does not leak (e.g., `self._imap_conn` set to None, context cleared)."
        },
        {
          "id": 5,
          "title": "Integrate AccountProcessor usage pattern and validate state isolation",
          "description": "Define the standard usage pattern for AccountProcessor, ensure it integrates with the surrounding system, and verify that processing for different accounts remains fully isolated.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1. In the part of the codebase that orchestrates multi-account processing (e.g., a manager or scheduler), integrate `AccountProcessor` so that each account gets its own instance constructed with its own configuration and dependencies.\n2. Establish the canonical call pattern, e.g., `processor = AccountProcessor(...); processor.setup(); processor.run(); processor.teardown()`, or the equivalent context-managed form, and update any existing orchestration code to follow this pattern.\n3. Add or update unit/integration tests to:\n   - Create two AccountProcessor instances with different account configs and verify their internal state (config, IMAP connection details, processing context) does not overlap or depend on one another.\n   - Simulate concurrent or sequential runs across accounts and confirm that blacklists, whitelists, LLM prompts, and generated notes remain account-specific.\n   - Verify that failures in one account’s `run()` do not affect the others beyond whatever error signaling the orchestrator expects.\n4. Refine logging to include an account identifier in all log messages from AccountProcessor, so logs can be correlated per account without implying shared state.\n5. Update any relevant documentation or README sections to describe how to instantiate and use AccountProcessor and to emphasize its per-account isolation guarantees."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=8 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 8 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 8 - Create Account Processor Class [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 9,
      "title": "Implement Safety Interlock with Cost Estimation",
      "description": "Create the safety mechanism to prevent accidental high-cost operations",
      "status": "done",
      "dependencies": [
        8
      ],
      "priority": "high",
      "details": "Implement a safety interlock system that: 1) Uses IMAP.search to count emails before fetching, 2) Calculates estimated cost based on email count and model cost from config, 3) Displays cost to user, 4) Requires explicit confirmation before proceeding. Add this to the AccountProcessor before email fetching.",
      "testStrategy": "Test with various email counts and model costs. Verify correct cost calculation and that processing stops without confirmation.",
      "subtasks": [
        {
          "id": 1,
          "title": "Add pre-fetch email counting using IMAP.search in AccountProcessor",
          "description": "Introduce a preliminary IMAP.search step in AccountProcessor to count candidate emails before any fetching occurs.",
          "status": "done",
          "dependencies": [],
          "details": "Identify the code path in AccountProcessor where email fetching currently begins (e.g., the method that first issues IMAP.fetch or equivalent). Before any fetch calls, insert a new step that uses IMAP.search with the same search criteria used for fetching to retrieve only the list of matching message IDs. From this list, compute the email count (length of IDs array). Ensure that this search result is not immediately fetched, and that the count and message ID list are stored in a suitable structure (e.g., a local variable or a small value object) that can be passed to subsequent steps in the processing flow."
        },
        {
          "id": 2,
          "title": "Implement cost estimation function based on email count and model config",
          "description": "Create a reusable helper that estimates operation cost from the email count and model pricing configuration.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Add a dedicated function (e.g., estimateCost(emailCount, modelConfig)) in an appropriate module or within AccountProcessor. This function should: 1) accept the email count from the IMAP.search step and 2) read model cost parameters from the existing configuration (e.g., per-1K-tokens or per-email cost, plus any fixed overhead). Define a clear formula for mapping email count to estimated cost (for example, assume an average tokens-per-email value from config or a hardcoded constant, then multiply by the per-token price). Return a structured result containing at least the numeric cost (e.g., as a float in the account’s currency) and any auxiliary information needed for UI display (e.g., breakdown components or formatted string). Ensure that no network calls are made here; this should be a pure calculation using already available config values."
        },
        {
          "id": 3,
          "title": "Display estimated cost and prompt user for confirmation",
          "description": "Integrate a user-facing confirmation step that shows the estimated cost and clearly asks for explicit approval before proceeding.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "In the flow after computing the cost estimate, inject a UI or interaction step that presents: 1) the number of emails to be processed, 2) the estimated cost calculated by the helper, and 3) a clear warning that this is a potentially high-cost operation. Implement a confirmation mechanism appropriate to the environment (e.g., a modal dialog, CLI prompt, or web form) with explicit options such as \"Confirm\" and \"Cancel\". Ensure the code path awaits or handles the user’s response and that no email fetching begins until a positive confirmation is received. Structure this so that the confirmation step returns a boolean (confirmed / not confirmed) to the calling logic."
        },
        {
          "id": 4,
          "title": "Wire safety interlock logic into AccountProcessor control flow",
          "description": "Treat the pre-count, cost estimation, and confirmation as a safety interlock that must pass before email fetching can execute.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Refactor AccountProcessor so that the main flow becomes: (a) run IMAP.search and obtain email count; (b) compute estimated cost via the cost estimation helper; (c) call the confirmation UI to obtain explicit user consent; (d) only if consent is granted, proceed to the existing email fetching logic using the previously obtained message IDs. If the user cancels, short-circuit the process gracefully: log that the operation was aborted by the safety interlock, and return an appropriate status to the caller without fetching or processing any emails. Ensure that the existing fetching logic does not run from any other code path that bypasses this interlock; if necessary, centralize fetch initiation behind a single method that enforces the interlock checks."
        },
        {
          "id": 5,
          "title": "Add configuration, thresholds, and tests for the safety interlock",
          "description": "Finalize the feature with configuration options, thresholds for what counts as high-cost, and automated tests covering the interlock behavior.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Extend configuration to include relevant safety interlock settings, such as: average tokens per email (if used), per-model pricing data, a threshold above which the interlock is enforced, and any flags to enable/disable the interlock for development. Implement logic so that if the estimated cost is below a low-risk threshold, the system can optionally skip the confirmation step, while still allowing forced confirmation for all operations if configured. Write automated tests that cover: (1) operations below the threshold proceed without confirmation (if allowed by config), (2) operations above the threshold show the cost and require confirmation, (3) canceling at the confirmation step prevents any IMAP.fetch calls, and (4) confirming leads to correct fetching of the expected message IDs. Include tests for edge cases like zero matching emails, misconfigured pricing, and large email counts."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=9 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 9 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 9 - Implement Safety Interlock with Cost Estimation [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 10,
      "title": "Create Master Orchestrator Class",
      "description": "Implement the MasterOrchestrator to manage multiple accounts",
      "status": "done",
      "dependencies": [
        9
      ],
      "priority": "high",
      "details": "Create src/orchestrator.py with a MasterOrchestrator class that: 1) Handles CLI arguments for account selection, 2) Iterates through selected accounts, 3) Creates isolated AccountProcessor instances for each account, 4) Manages the overall processing flow. Ensure proper error handling if one account fails.",
      "testStrategy": "Test with multiple account configurations. Verify isolation between accounts and proper error handling when one account has issues.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define MasterOrchestrator class structure and dependencies",
          "description": "Create src/orchestrator.py, define the MasterOrchestrator class skeleton, and wire in required dependencies such as AccountProcessor and logging.",
          "status": "done",
          "dependencies": [],
          "details": "1) Create the file src/orchestrator.py if it does not exist.\n2) Add necessary imports: argparse (or click/typer depending on project standard) for CLI handling, logging, typing (List, Optional, etc.), and the AccountProcessor class from its module.\n3) Define a MasterOrchestrator class with an __init__ method that accepts configuration needed to discover available accounts (e.g., a list of account identifiers or a config object) and an optional logger instance.\n4) Inside __init__, store configuration and initialize a logger (use the project’s logging convention, falling back to Python’s logging.getLogger(__name__) if none is provided).\n5) Add placeholder methods with clear signatures (and docstrings) for: parse_args, select_accounts, create_account_processor, and run (overall processing flow), leaving implementations as pass for now."
        },
        {
          "id": 2,
          "title": "Implement CLI argument parsing for account selection",
          "description": "Implement argument parsing logic in MasterOrchestrator to handle account selection from the command line.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) In MasterOrchestrator, implement a parse_args(cls, argv: Optional[List[str]] = None) -> argparse.Namespace classmethod or a standalone function, depending on project style.\n2) Define CLI options for selecting accounts, e.g. --account <id> (repeatable), --accounts <id1,id2,...>, or --all-accounts, aligning with existing CLI conventions in the project.\n3) Validate that at least one form of selection is provided, or define a sensible default (e.g., all accounts) if none is specified.\n4) Normalize parsed arguments into a canonical representation (e.g., a list of account IDs) and expose this to the rest of the orchestrator, either by returning the list directly from a helper (get_selected_accounts_from_args) or by setting an instance attribute like self.selected_accounts.\n5) Ensure helpful error messages and --help text clearly explain how to select accounts."
        },
        {
          "id": 3,
          "title": "Implement account selection and iteration mechanism",
          "description": "Convert parsed CLI input into a concrete list of accounts and implement iteration over those accounts within MasterOrchestrator.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1) Implement a select_accounts(self, args) -> List[str] (or appropriate account identifier type) method that uses the parsed CLI arguments to determine which accounts to process.\n2) If accounts are defined in a config file, environment, or database, resolve CLI-specified identifiers to concrete account configurations, raising a clear error if an unknown account is requested.\n3) Store the resulting list on the orchestrator instance (e.g., self.accounts_to_process) to be used by the processing loop.\n4) Implement a private iterator-like helper (e.g., _iter_accounts(self)) that yields each account entry in turn; this encapsulates any future filtering, ordering, or batching logic.\n5) Add basic logging to indicate which accounts have been selected before processing begins."
        },
        {
          "id": 4,
          "title": "Create isolated AccountProcessor instances and per-account execution",
          "description": "For each selected account, create a fresh AccountProcessor instance with isolated state and execute its processing logic.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "1) Implement create_account_processor(self, account) -> AccountProcessor, which instantiates AccountProcessor with only the data/config for that single account (no shared mutable state between accounts).\n2) Ensure that any shared services (e.g., HTTP clients, DB connections) are either safely shared or cleanly encapsulated per AccountProcessor, based on project requirements.\n3) Define a clear interface contract for AccountProcessor (e.g., a run() or process() method that performs all necessary work for that account) and use that consistently in MasterOrchestrator.\n4) In the orchestrator’s main loop (to be implemented in run), call create_account_processor for each account and immediately invoke the processor’s execution method, avoiding reuse of AccountProcessor instances across accounts.\n5) Add logging at the start and end of each account’s processing to make per-account execution traceable."
        },
        {
          "id": 5,
          "title": "Implement overall processing flow with robust error handling",
          "description": "Implement the MasterOrchestrator.run method to coordinate the full flow—from argument parsing to per-account processing—while ensuring that failures in one account do not stop others.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "1) Implement MasterOrchestrator.run(self, argv: Optional[List[str]] = None) as the main entrypoint: parse CLI args, select accounts, and orchestrate their processing.\n2) Inside run, wrap the per-account processing loop in try/except blocks so that an exception in one account is caught, logged (with stack trace), and recorded, but does not prevent the loop from continuing with remaining accounts.\n3) Decide on and implement an error aggregation strategy (e.g., collect failed account IDs and associated error messages in a list or dict) and return an overall status or raise a summarized exception at the end if required by the project.\n4) Ensure that any necessary cleanup per account (e.g., closing connections) is handled via finally blocks or context managers so cleanup occurs even on failure.\n5) Provide a small top-level integration point (e.g., a main() function in orchestrator.py or usage from the project’s main CLI) that constructs MasterOrchestrator and calls run, to validate end-to-end behavior."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=10 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 10 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 10 - Create Master Orchestrator Class [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 11,
      "title": "Update CLI for Multi-Account Support",
      "description": "Enhance the CLI to support account-specific commands",
      "status": "done",
      "dependencies": [
        10
      ],
      "priority": "medium",
      "details": "Update the CLI module to support new commands: 'process --account <name>', 'process --all', and 'show-config --account <name>'. Integrate with the MasterOrchestrator for execution. Ensure proper argument validation and help text.",
      "testStrategy": "Test each command with various arguments including edge cases like invalid account names. Verify proper integration with the MasterOrchestrator.",
      "subtasks": [
        {
          "id": 1,
          "title": "Extend CLI argument parsing to support new multi-account flags",
          "description": "Add and wire up the new CLI options `process --account <name>`, `process --all`, and `show-config --account <name>` in the existing CLI module.",
          "status": "done",
          "dependencies": [],
          "details": "Identify where CLI commands and flags are currently defined (e.g., argparse/Click/subcommand registry) and extend the `process` and `show-config` commands to accept the new flags.\n- For `process`, add mutually exclusive options `--account <name>` and `--all` alongside any existing options.\n- For `show-config`, add the `--account <name>` option.\n- Ensure the parsed arguments structure (e.g., options object) includes fields for `accountName`, `allAccounts`, etc., that can be consumed downstream.\n- Update any command routing or dispatch switch statements so that invocations including these options reach the appropriate handler functions without changing behavior of existing commands."
        },
        {
          "id": 2,
          "title": "Implement argument validation rules for account-related options",
          "description": "Define and enforce validation rules for the new account-related flags to ensure correct and unambiguous CLI usage.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Implement validation logic that runs immediately after parsing and before invoking business logic.\n- Enforce that `process` must receive exactly one of: `--account <name>` or `--all`, and fail with a clear error message if neither or both are provided.\n- For `show-config`, require `--account <name>` if that is mandatory, or define precedence with existing options if not.\n- Validate that `<name>` is non-empty and adheres to any existing account naming rules (e.g., allowed characters, length); centralize these checks in a helper function for reuse.\n- Ensure validation errors exit with a non-zero status code and print concise, user-friendly messages that indicate the invalid combination and how to fix it.\n- Add unit tests (or extend existing ones) to cover valid and invalid combinations of the new flags."
        },
        {
          "id": 3,
          "title": "Integrate account-aware commands with MasterOrchestrator",
          "description": "Connect the new CLI command variants to the MasterOrchestrator so that account-specific and all-account processing is executed correctly.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "Update the command handlers to call the MasterOrchestrator with the appropriate parameters derived from the parsed and validated arguments.\n- For `process --account <name>`, call a MasterOrchestrator method that processes a single account (e.g., `orchestrator.processAccount(accountName)`), adding such a method if it does not exist yet.\n- For `process --all`, call a method that loops through all known accounts (e.g., `orchestrator.processAllAccounts()`), implemented either directly in MasterOrchestrator or as a helper that enumerates available accounts and invokes the per-account logic.\n- For `show-config --account <name>`, invoke a MasterOrchestrator (or related service) method to fetch and display that account’s configuration.\n- Ensure that orchestration errors are surfaced to the CLI with proper exit codes and messages while not leaking internal stack traces in normal error output.\n- Add tests (or integration-style checks) to verify that the correct MasterOrchestrator methods are invoked for each flag combination."
        },
        {
          "id": 4,
          "title": "Update and refine CLI help text and usage documentation",
          "description": "Improve the CLI help output and related in-tool documentation to clearly describe the new multi-account options and their usage patterns.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "Modify the help/usage strings associated with the `process` and `show-config` commands.\n- For `process`, document the purpose of `--account <name>` and `--all`, including that they are mutually exclusive and at least one is required, with concrete examples.\n- For `show-config`, document how `--account <name>` works and what information is shown.\n- Ensure the global `--help` output and any command-specific `--help` reflect the new options and validation rules.\n- If there is separate README or CLI reference documentation in the repo, update it to include example invocations and explain common workflows (single account vs all accounts)."
        },
        {
          "id": 5,
          "title": "Add comprehensive tests and perform regression verification",
          "description": "Create or extend automated tests to cover new behaviors and verify that existing CLI functionality remains unaffected.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Implement test cases for the new multi-account commands using the project’s existing testing framework.\n- Add tests for `process --account <name>` and `process --all`, ensuring correct argument parsing, validation, orchestration calls, and exit codes.\n- Add tests for `show-config --account <name>`, verifying correct parsing and that the configuration for the specified account is retrieved and displayed.\n- Include negative tests for invalid flag combinations and missing required options, asserting the exact error messages and non-zero exit codes.\n- Run the full test suite (and any available CLI integration tests) to confirm no regressions in existing commands.\n- Adjust or add mocks/stubs for MasterOrchestrator as necessary to isolate CLI behavior in the tests."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=11 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 11 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 11 - Update CLI for Multi-Account Support [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement Enhanced Logging System",
      "description": "Create a comprehensive logging system for multi-account processing",
      "status": "done",
      "dependencies": [
        10
      ],
      "priority": "medium",
      "details": "Implement an enhanced logging system that: 1) Overrides logging on startup, 2) Logs account processing start/end, 3) Logs configuration overrides, 4) Includes context in all log messages. Create a centralized logging configuration that can be used throughout the application.",
      "testStrategy": "Verify log output for various processing scenarios. Check that account context is properly included in logs and that configuration overrides are clearly logged.",
      "subtasks": [
        {
          "id": 1,
          "title": "Design centralized logging configuration and context schema",
          "description": "Define a unified logging configuration and the standard context fields that will be attached to all log messages across the multi-account processing application.",
          "status": "done",
          "dependencies": [],
          "details": "• Choose the logging framework (e.g., language-standard logging library plus any structured logging/JSON formatter) and decide on log format (JSON vs plain text), log levels, and handlers/appenders (console, file, remote).\n• Design a common log context schema including fields such as correlation_id, account_id, job_id, component/module, environment, and request_id so every log line can be tied back to a specific account and processing run.\n• Define logger naming conventions (e.g., per module/component) and how child loggers inherit configuration from a root logger.\n• Document expectations for log level usage (DEBUG/INFO/WARN/ERROR) and what should/should not be logged (e.g., no secrets, PII handling rules).\n• Output of this subtask should be a short spec/config document and, if applicable, a base logging config file/template (e.g., logging.yaml/logging.json or equivalent) that will be referenced by the implementation subtasks."
        },
        {
          "id": 2,
          "title": "Implement startup-time logging override and centralized configuration loader",
          "description": "Create a centralized mechanism that initializes and overrides the application’s logging configuration on startup so that all components use the same enhanced logging settings.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "• Implement a logging bootstrap module (e.g., logging_config.py or LoggingInitializer class) that reads the configuration defined in subtask 1 (file, env vars, or code) and applies it to the root logger as the first step of application startup.\n• Ensure this initializer can be invoked from the main entrypoint(s) so that any pre-existing/default logging configuration is overridden (e.g., reset existing handlers and reconfigure with the centralized config).\n• Provide a single public function (e.g., init_logging(config_path=None, overrides=None)) that other parts of the app can call to set up logging, allowing runtime overrides such as log level, output destination, or format.\n• Verify that all modules obtain loggers via the standard framework (e.g., `logging.getLogger(__name__)`) after the initializer runs, ensuring they inherit the centralized configuration.\n• Add minimal tests or a small test harness to confirm that startup logs follow the new format and settings and that changing configuration is reflected globally."
        },
        {
          "id": 3,
          "title": "Implement contextual logging utilities for multi-account processing",
          "description": "Create helper utilities to attach and propagate contextual information (account, correlation IDs, etc.) so that all log messages include the required context automatically.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "• Implement a context mechanism appropriate for the stack (e.g., MDC/logging filters, thread-local/contextvars storage, or a wrapper logger) that can store fields like account_id, job_id, correlation_id, and environment and inject them into all log records.\n• Create lightweight helper functions or classes (e.g., `with_account_context(account_id)`, `set_correlation_id(id)`) that set/clear context for the current execution scope (thread, async task, request, or batch run).\n• Configure the logging formatter to include these context fields in every log line (for JSON, add them as keys; for text, add them to the pattern).\n• Ensure that context propagates correctly across internal calls and, if applicable, across async boundaries or worker threads.\n• Add unit tests or small integration checks to validate that when context is set, any log emitted from that flow includes the correct context fields and that context is cleared or replaced between different accounts/runs."
        },
        {
          "id": 4,
          "title": "Add structured logging for account processing lifecycle and configuration overrides",
          "description": "Integrate logging into the multi-account processing flows to record processing start/end for each account and to log configuration overrides in a structured, consistent way.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "• Identify the entrypoints for multi-account processing (e.g., account processing loop, worker, or job orchestrator) and wrap each account’s run with logs at INFO level for start and end, using the contextual logging utilities to include account_id and correlation_id.\n• Ensure that errors/exceptions during account processing are logged with ERROR level and include the same contextual fields, plus any relevant diagnostic data (stack trace, failing step).\n• Implement structured logging of configuration overrides: whenever runtime configuration differs from defaults (CLI flags, env vars, DB settings), log a single well-structured record containing which options were overridden, their effective values, scope (global vs per-account), and source.\n• Make sure these lifecycle and configuration logs use the centralized logging configuration (no ad-hoc print statements or local logger setups) and follow naming/level conventions.\n• Add tests or a test run script that simulates processing multiple accounts and verifies that logs clearly show per-account start/end and configuration overrides, with correct context and format."
        },
        {
          "id": 5,
          "title": "Refactor application components to adopt centralized logging and validate end-to-end behavior",
          "description": "Update existing modules to use the new centralized and contextual logging system exclusively, and verify that logging works correctly across the entire multi-account processing flow.",
          "status": "done",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "• Search the codebase for all direct logging usages (ad-hoc configurations, print statements, custom loggers) and refactor them to use the centralized initializer and contextual logging utilities created in previous subtasks.\n• Remove or disable any module-level logging configuration that conflicts with the centralized configuration, ensuring there is exactly one place where logging is configured on startup.\n• Standardize log messages to be concise, level-appropriate, and context-rich; avoid logging sensitive data while preserving enough information for debugging multi-account runs.\n• Execute an end-to-end test or staging run that processes multiple accounts, then inspect the logs to confirm: startup configuration is applied, every message includes context, account start/end are logged, configuration overrides appear as designed, and log volume/noise is acceptable.\n• Based on findings, make minor tuning adjustments (e.g., log levels, additional context fields, throttling of very verbose sections) and update documentation so future developers know how to use the enhanced logging system."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=12 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 12 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 12 - Implement Enhanced Logging System [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 13,
      "title": "Add Progress Bars for Processing Steps",
      "description": "Implement progress indicators for long-running operations",
      "status": "done",
      "dependencies": [
        11,
        12
      ],
      "priority": "low",
      "details": "Add progress bars for email processing steps using a library like tqdm. Implement progress tracking for: email fetching, content parsing, LLM processing, and note generation. Ensure progress bars work correctly with multi-account processing.",
      "testStrategy": "Test progress bar display with various email counts. Verify accurate progress reporting and proper cleanup after completion.",
      "subtasks": [
        {
          "id": 1,
          "title": "Introduce tqdm dependency and progress configuration utilities",
          "description": "Add tqdm as a dependency and create a small utility layer to standardize progress bar creation and configuration across the project.",
          "status": "done",
          "dependencies": [],
          "details": "1. Add `tqdm` to the project dependencies (e.g., requirements.txt/pyproject.toml) and ensure it is installed in the dev environment.\n2. Create a module such as `progress.py` (or similar) that wraps tqdm usage, e.g.:\n   - `from tqdm.auto import tqdm`\n   - Helper functions like `create_progress_bar(total: int, desc: str, unit: str = \"items\") -> tqdm`.\n3. In this utility, centralize common settings (e.g., `mininterval`, `ncols`, `disable` flag from a config env var, consistent units).\n4. Expose helper functions for both iterable-based and manual-update scenarios, e.g.:\n   - `with tqdm(total=total, desc=desc) as pbar: ... pbar.update(n)` for non-iterable processes.\n5. Ensure the utilities are non-UI-blocking (no input) and work both in CLI and notebook environments by using `tqdm.auto`."
        },
        {
          "id": 2,
          "title": "Add progress bars for email fetching per account",
          "description": "Instrument the email fetching logic with tqdm-based progress bars, showing progress per account and overall where possible.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1. Identify the function(s) responsible for fetching emails, e.g., `fetch_emails_for_account(account)` and any higher-level coordinator that loops over accounts.\n2. Where a list/iterator of email IDs or messages is already available (e.g., after listing messages for an account), wrap the loop with tqdm from the progress utility:\n   - `for msg in tqdm(messages, desc=f\"Fetching emails ({account.name})\", unit=\"emails\"):`.\n3. If you fetch emails in chunks or without a known total, create a manual tqdm with `total` set once you know the count, or omit `total` and allow tqdm to infer from the iterable length.\n4. If multi-account fetching is done in a loop, add an outer progress bar for accounts: `for account in tqdm(accounts, desc=\"Accounts\", unit=\"acct\"):` and keep the inner bar for per-account emails.\n5. Ensure progress bars do not break existing logging; use `tqdm.write()` for log lines inside loops to avoid mangling the bar."
        },
        {
          "id": 3,
          "title": "Add progress bars for email content parsing",
          "description": "Track progress while parsing email content for each account, displaying the number of emails parsed and time estimates.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1. Locate the parsing pipeline, e.g., `parse_email_contents(emails)` or similar batch parsing step following fetching.\n2. Wrap the main parsing loop with tqdm from the progress utility, using a clear description and units, e.g.:\n   - `for email in tqdm(emails, desc=f\"Parsing content ({account.name})\", unit=\"emails\"):`.\n3. If parsing is done in multiple stages (e.g., header, body, attachments), consider either:\n   - a single progress bar over emails with inner non-progress loops, or\n   - a single bar with more granular `desc` updates if needed.\n4. For streaming or generator-based parsing, use a manual `total` if the email count is known in advance; otherwise allow dynamic statistics without ETA.\n5. Make sure parsing progress bars are created per account (reusing the same pattern used in fetching) and that they appear logically after any fetching bars to avoid visual clutter."
        },
        {
          "id": 4,
          "title": "Add progress bars for LLM processing of emails",
          "description": "Instrument the LLM processing step with progress bars to show how many emails (or chunks) have been processed by the language model.",
          "status": "done",
          "dependencies": [
            1,
            3
          ],
          "details": "1. Find the component that calls the LLM for each email or batch, e.g., `run_llm_on_emails(emails)`.\n2. Wrap the primary loop that issues LLM calls with a tqdm bar, e.g.:\n   - `for item in tqdm(email_items, desc=f\"LLM processing ({account.name})\", unit=\"items\"):` where `items` may be emails, threads, or chunks.\n3. If the LLM is called asynchronously or in parallel, still track completions by updating the bar when a task finishes, e.g.:\n   - Initialize `pbar = create_progress_bar(total=len(email_items), desc=...)`.\n   - In each callback/future completion handler, call `pbar.update(1)`.\n4. Ensure the LLM progress bar is compatible with any retry logic and does not over-count retries (only update on successful or final outcome per item).\n5. Where multi-account processing loops exist, either:\n   - show one LLM bar per account (created inside the per-account function), or\n   - include account name in the `desc` so bars are distinguishable when multiple are shown."
        },
        {
          "id": 5,
          "title": "Add progress bars for note generation and ensure correct behavior with multi-account processing",
          "description": "Track progress for note generation based on LLM outputs and verify that all progress bars (fetching, parsing, LLM, notes) behave correctly when processing multiple accounts.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1. Identify the function that converts LLM outputs into notes, e.g., `generate_notes_from_results(results)` or per-email note creation.\n2. Wrap the note generation loop with tqdm, e.g.:\n   - `for result in tqdm(llm_results, desc=f\"Note generation ({account.name})\", unit=\"notes\"):`.\n3. If note generation is chained directly after LLM processing within a single loop, either:\n   - keep a single bar counting emails fully processed, or\n   - create a separate bar only if the note-phase is long enough to warrant visibility.\n4. Test the full pipeline with multiple accounts (including edge cases: zero emails, few emails, many emails) to ensure:\n   - Progress bars appear in a stable, non-jumbled order.\n   - Per-account bars reset correctly and do not leak state between accounts.\n   - Nested bars (accounts vs per-step) remain readable; adjust `desc`, `leave`, or `position` parameters if needed.\n5. Update any developer documentation or README to describe the new progress behavior, including how to enable/disable bars (e.g., via config) and how they behave in multi-account runs."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=13 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 13 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 13 - Add Progress Bars for Processing Steps [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 14,
      "title": "Create Default Configuration Templates",
      "description": "Develop default configuration files for global and account settings",
      "status": "done",
      "dependencies": [
        3
      ],
      "priority": "medium",
      "details": "Create template configuration files: 1) Default global config.yaml with all settings, 2) Example account configuration with common overrides, 3) Template blacklist.yaml and whitelist.yaml with example rules. Include detailed comments explaining each setting and its purpose.",
      "testStrategy": "Validate all template files against the configuration schema. Verify they can be loaded without errors and produce expected merged configurations.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define configuration schema and directory layout",
          "description": "Identify all global and account-level configuration settings, their data types, defaults, and relationships, and decide on the directory structure and file naming conventions for config.yaml, blacklist.yaml, and whitelist.yaml.",
          "status": "done",
          "dependencies": [],
          "details": "1) List all configuration domains (e.g., logging, authentication, rate limiting, feature flags, integrations). 2) For each domain, enumerate individual settings, valid values, and default behaviors. 3) Distinguish which settings belong in the **global config** vs. which are **account overrides**; document precedence rules (e.g., account > global). 4) Define the YAML structure for each file: key nesting, section names, and how rules are expressed in blacklist/whitelist files. 5) Decide on directory layout (e.g., `/config/global/config.yaml`, `/config/accounts/example_account.yaml`, `/config/rules/blacklist.yaml`, `/config/rules/whitelist.yaml`) and file naming patterns. 6) Capture this as a short schema/ADR document to be used as the reference for subsequent template creation."
        },
        {
          "id": 2,
          "title": "Create default global config.yaml template with full settings",
          "description": "Implement the default global config.yaml template that includes all known settings, structured according to the schema, with safe defaults and detailed comments explaining each setting and its purpose.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Create `config/global/config.yaml` (or agreed path) using the structure defined in the schema. 2) Add all global settings as keys with sensible, secure defaults (e.g., non-destructive, minimal-permission defaults where applicable). 3) Group settings into logical sections with clear top-level comments describing each section. 4) For each setting, add an inline or preceding comment describing: what it does, accepted values and units, default behavior, example uses, and any security/performance implications. 5) Ensure comments explain how this global value interacts with account overrides and rule files. 6) Validate the YAML syntax (via linter or existing config loader) and adjust formatting for readability and consistency (indentation, quoting, list formatting)."
        },
        {
          "id": 3,
          "title": "Create example account configuration template with common overrides",
          "description": "Develop an example account-level configuration YAML that demonstrates typical overrides of global settings and documents how per-account configuration should be used.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) Create `config/accounts/example-account.yaml` (or agreed name) mirroring the relevant parts of the global schema. 2) Populate it with realistic example values that differ from the global defaults to clearly illustrate override behavior (e.g., higher rate limits, different feature flags, custom logging level). 3) Add comments at the top explaining the precedence model (how and when account config overrides global config) and how to create additional account configs (naming and placement). 4) For each overridden setting, add comments explaining why you might override it and any constraints (e.g., maximum allowed values, environment-specific notes). 5) Include at least one example of an account-specific setting that does not appear in the global config if the design allows such extensions, and document how the system handles unknown/extra keys. 6) Validate YAML syntax and ensure comment style, formatting, and terminology are consistent with the global config template."
        },
        {
          "id": 4,
          "title": "Create blacklist.yaml and whitelist.yaml rule templates with examples",
          "description": "Implement template blacklist.yaml and whitelist.yaml files containing example rule definitions that cover common use cases and clearly document rule structure, matching semantics, and evaluation order.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Create `config/rules/blacklist.yaml` and `config/rules/whitelist.yaml` (or agreed paths). 2) Based on the schema, define the top-level structure for rules (e.g., lists of rule objects with fields such as `id`, `type`, `pattern`, `condition`, `enabled`, `priority`, `description`). 3) Add multiple example rules in each file to demonstrate different rule types and conditions (e.g., blocking by IP, account ID, domain, feature, or action; whitelisting specific exceptions). 4) In comments, document how rules are evaluated: precedence of whitelist vs. blacklist, rule priority fields, first-match vs. all-match behavior, and what happens on no match. 5) Explain the syntax for patterns (literal vs. regex/glob), case sensitivity, and any performance/security considerations (e.g., avoid overly broad regex). 6) Include guidance comments on how to disable rules safely, how to add new ones, and any naming or ID conventions required by the system. 7) Validate YAML syntax and keep formatting consistent with the other config templates."
        },
        {
          "id": 5,
          "title": "Add cross-file documentation, usage notes, and integration checks",
          "description": "Ensure all template files are coherently documented, reference each other where relevant, and are validated against the intended loader/validation logic so they can be used as reliable starting points.",
          "status": "done",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "1) Add a short header comment to each template file describing its role, how it interacts with the other configuration files, and where it should be placed in a typical deployment. 2) Cross-reference related files in comments (e.g., in global config mention that blacklist/whitelist rules further refine behavior; in account config mention which global sections are most commonly overridden). 3) If a schema or validation tool exists, run each template through it and fix any inconsistencies; if not, at least load them with the application’s config loader in a test environment to ensure compatibility. 4) Where there are implicit conventions (e.g., required keys, naming of accounts, or rule IDs), document these explicitly in comments or a brief `README` comment block at the top of the main template. 5) Finalize formatting and comment style for consistency across all templates, and ensure that the templates are checked into version control in an appropriate location, ready for other developers or operators to copy and customize."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=14 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 14 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 14 - Create Default Configuration Templates [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 15,
      "title": "Implement Configuration Display Command",
      "description": "Add functionality to display merged configuration for debugging",
      "status": "done",
      "dependencies": [
        11,
        14
      ],
      "priority": "low",
      "details": "Implement the 'show-config --account <name>' command to display the merged configuration for an account. Format the output for readability, highlighting which values are overridden from the global config. Include options for different output formats (YAML, JSON).",
      "testStrategy": "Test the command with various account configurations. Verify correct display of merged configuration and clear indication of overridden values.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define CLI interface and options for `show-config` command",
          "description": "Add the `show-config --account <name>` command to the CLI, including options for output formats and any future-proof flags.",
          "status": "done",
          "dependencies": [],
          "details": "1) Extend the existing CLI/command parser to register a new `show-config` command (or subcommand, depending on current CLI design) that accepts a required `--account <name>` argument. 2) Add optional flags for output format, e.g. `--format yaml` and `--format json`, with a sensible default (likely YAML). 3) If the CLI has a shared options system, reuse existing pattern for enumeration/validation of option values. 4) Ensure help/usage text clearly documents the command’s purpose ('display merged configuration for debugging'), required arguments, and available formats. 5) Wire the command handler to a new function signature (e.g. `run_show_config(accountName: string, format: OutputFormat)`), leaving the implementation stubbed for now."
        },
        {
          "id": 2,
          "title": "Implement configuration merge logic for global and account scopes",
          "description": "Create a reusable function that computes the merged configuration for a given account by overlaying account-specific config on top of global defaults.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Identify where global configuration and per-account configuration are currently loaded/stored (e.g. config service, files, or in-memory structs). 2) Implement a function such as `getMergedConfig(accountName: string) -> MergedConfig` that: a) retrieves global config; b) retrieves config for the given account; c) overlays account-specific values onto the global config using a well-defined precedence rule (account overrides global); d) handles missing account or global configs gracefully with clear error behavior. 3) Ensure the merge is recursive for nested structures (maps/objects), i.e. only overridden fields are replaced and other global values are preserved. 4) Decide how to handle lists/arrays (replace entirely vs. merge items) and document this behavior. 5) Add unit tests covering cases: no account config, full override account config, partial overrides, and invalid/non-existing account names."
        },
        {
          "id": 3,
          "title": "Design data structure to track overridden vs inherited values",
          "description": "Extend the merged configuration representation to annotate which values are overridden from global config versus inherited, enabling highlighting in output.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "1) Define an internal data model for the merged config that can carry metadata per field, e.g. a wrapped type like `{ value, source }` or a parallel structure (e.g. a tree of flags) indicating `source: 'global' | 'account'`. 2) Adapt `getMergedConfig` (or add a variant like `getAnnotatedMergedConfig`) to produce this annotated structure during the merge instead of a plain config object. 3) Ensure the annotation logic is recursive, with clear rules for nested objects and collections (e.g. each key or item gets its own `source`). 4) Consider performance and memory implications; if needed, keep annotations only until rendering (do not persist them). 5) Add tests that assert correct source tagging for simple and nested fields, including edge cases where account config explicitly sets a value equal to the global value (decide whether that is still considered an override and document the rule)."
        },
        {
          "id": 4,
          "title": "Implement formatted output and highlighting of overrides",
          "description": "Create the rendering layer for the annotated merged configuration, supporting human-readable YAML/JSON output and visually highlighting overridden values.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "1) Implement a formatter module with functions such as `renderMergedConfigYaml(annotatedConfig)` and `renderMergedConfigJson(annotatedConfig)`. 2) For **YAML**: a) serialize the config with stable key ordering for readability; b) apply highlighting of overridden values, e.g. via inline comments (`# overridden from global`), color codes/ANSI styles if your CLI supports them, or a prefix marker convention; c) ensure the resulting YAML is still valid if copied, i.e. comments and ANSI codes do not break structure. 3) For **JSON**: a) keep output valid JSON; b) embed override information either as: i) additional `__source` fields if acceptable; or ii) an optional `--with-sources` flag that prints a secondary structure, or iii) omit inline highlighting and rely on a separate note (choose the pattern consistent with existing tooling and document it). 4) Provide a clear, consistent legend in the CLI output header (e.g. a short explanation of how overrides are indicated). 5) Write focused tests (or golden-file snapshots) for both YAML and JSON outputs to ensure deterministic formatting and correct rendering of override markers."
        },
        {
          "id": 5,
          "title": "Wire command handler, error handling, and tests for `show-config`",
          "description": "Connect the CLI command to the merge and formatting logic, handle error cases, and add end-to-end tests for `show-config --account`.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "1) Implement the `show-config` command handler to: a) parse `--account` and `--format`; b) call the annotated merge function for the given account; c) select the correct formatter based on `--format`; d) print the result to stdout. 2) Add robust error handling: a) unknown account name (clear error message and non-zero exit code); b) config loading/IO errors; c) invalid format option; d) unexpected internal errors with a generic but informative message. 3) If applicable, add a `--verbose` or `--debug` flag that prints diagnostic details (e.g. which config files were loaded) without changing the core output format. 4) Implement integration or CLI tests that invoke the binary/command (or the top-level handler) for scenarios: valid account (YAML), valid account (JSON), non-existent account, partial account config, and overrides present. 5) Update any developer-facing documentation or README sections describing available CLI commands, including usage examples for `show-config --account <name> --format yaml|json` and explanation of how overrides are indicated."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=15 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 15 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 15 - Implement Configuration Display Command [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 16,
      "title": "Create Unit Tests for Core Components",
      "description": "Develop comprehensive unit tests for all major components",
      "status": "done",
      "dependencies": [
        8,
        10
      ],
      "priority": "medium",
      "details": "Create unit tests for all core components: ConfigLoader, Rules Engine, Content Parser, AccountProcessor, and MasterOrchestrator. Use mocking to isolate components and test specific behaviors. Aim for high test coverage of critical functionality.",
      "testStrategy": "Run tests with coverage reporting. Verify all critical code paths are tested, including error handling and edge cases.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up unit test project structure and testing utilities",
          "description": "Create or validate the test project/module setup and shared utilities needed to unit test the core components with mocks and high coverage.",
          "status": "done",
          "dependencies": [],
          "details": "1) Ensure the testing framework (e.g., Jest, JUnit, NUnit, pytest, etc.) is configured and runnable via the project’s build/CI pipeline. 2) Create a clear folder/package structure that mirrors the production code for ConfigLoader, RulesEngine, ContentParser, AccountProcessor, and MasterOrchestrator (e.g., tests/config/, tests/rules/, etc.). 3) Introduce common test utilities: factories/builders for common domain objects, test data generators, helper assertions, and reusable setup/teardown logic. 4) Configure and document a mocking framework appropriate to the language (e.g., Mockito, unittest.mock, Moq, sinon), including conventions on when to use mocks vs stubs vs fakes to isolate dependencies.[3][4] 5) Enable and verify code coverage reporting, and define target coverage thresholds for critical paths of the core components.[2][3]"
        },
        {
          "id": 2,
          "title": "Implement unit tests for ConfigLoader",
          "description": "Write isolated unit tests for ConfigLoader covering loading, parsing, validation, and error handling logic using mocks for external resources.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Identify all public methods and key behaviors of ConfigLoader: loading from files/ENV/remote sources, parsing configuration formats, default value handling, and validation rules. 2) For each behavior, design small, focused tests that each verify a single outcome (e.g., successful load of valid config, missing key fallback, invalid format error).[1][2][3] 3) Use mocks/stubs for any file system, environment, network, or secret store access so tests do not depend on real infrastructure.[3][4] 4) Cover positive and negative scenarios, including malformed configs, missing mandatory fields, unsupported versions, and boundary values.[2][3] 5) Assert that ConfigLoader surfaces appropriate exceptions or error objects and that log/diagnostic hooks are invoked where applicable. 6) Check coverage and add tests for untested branches in conditionals and error paths until target coverage for ConfigLoader is reached.[2][3]"
        },
        {
          "id": 3,
          "title": "Implement unit tests for RulesEngine and ContentParser",
          "description": "Develop unit tests for the business logic components RulesEngine and ContentParser, focusing on rule evaluation, parsing correctness, and edge cases with isolated dependencies.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) For RulesEngine: enumerate rule types, conditions, and outcomes; create parameterized tests where possible to run the same logic against multiple input rule sets and data payloads.[3] 2) Mock external services, repositories, and any time/random sources to keep tests deterministic and isolated.[2][3][4] 3) Write tests covering: correct rule matching, precedence/ordering, default/fallback rules, rule conflicts, and error handling when rules are malformed or incomplete. 4) For ContentParser: identify supported input formats and transformations; test nominal cases, malformed input, encoding issues, and boundary sizes. 5) Use concise, descriptive test names to document behavior (e.g., test_applies_highest_priority_matching_rule, test_parser_raises_error_on_invalid_json).[3][4] 6) Ensure both components have high coverage for branching logic and typical/edge scenarios; refactor or extend tests where coverage tools indicate gaps, prioritizing business‑critical paths.[2][3]"
        },
        {
          "id": 4,
          "title": "Implement unit tests for AccountProcessor",
          "description": "Create unit tests for AccountProcessor focusing on account lifecycle flows, calculations, and integration points, using mocks to simulate dependencies.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "1) Map the main responsibilities of AccountProcessor: creation/update flows, validation, state transitions, and financial or quota calculations. 2) For each flow, design tests that verify a single responsibility per test (e.g., test_creates_account_with_defaults, test_rejects_account_with_invalid_email, test_updates_balance_on_transaction).[1][2] 3) Mock dependencies such as repositories, messaging/queues, payment gateways, and other services so that tests run purely in memory without external systems.[3][4] 4) Cover both success and failure paths: invalid inputs, failed dependency calls, and rollback/compensation behavior if defined.[2][3] 5) Use builders/factories from the shared test utilities to create realistic but synthetic account objects and related data. 6) Review coverage to ensure all critical business rules and edge cases for AccountProcessor are exercised, adding tests for uncovered branches and error handling.[2][3]"
        },
        {
          "id": 5,
          "title": "Implement unit tests for MasterOrchestrator and finalize coverage",
          "description": "Write unit tests for MasterOrchestrator that verify orchestration logic and interactions with core components using mocks, and then tune the suite to meet coverage and quality goals.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "1) Identify the orchestration scenarios handled by MasterOrchestrator: execution pipelines, sequencing of ConfigLoader, RulesEngine, ContentParser, and AccountProcessor, and error/timeout handling. 2) Replace all real component dependencies with mocks/spies to verify that the orchestrator calls them in the correct order with the correct arguments and handles their returned results and failures appropriately.[3][4] 3) Write scenario-based tests for success pipelines, partial failures, retries (if any), and short‑circuit behavior when prerequisites fail. 4) Assert side effects such as emitted events, status updates, or aggregate results produced by MasterOrchestrator, while keeping each test limited to one main behavior.[1][3] 5) Run coverage analysis for the entire core component set; add or refine tests to cover remaining critical branches or flaky areas, focusing on quality rather than only percentage.[2][6] 6) Document key test scenarios and conventions in a brief testing README to aid future maintenance and reviews.[2]"
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=16 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 16 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 16 - Create Unit Tests for Core Components [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 17,
      "title": "Implement Integration Tests",
      "description": "Create tests that verify component interactions",
      "status": "done",
      "dependencies": [
        16
      ],
      "priority": "medium",
      "details": "Develop integration tests that verify the interaction between components: 1) ConfigLoader with AccountProcessor, 2) Rules Engine with the processing pipeline, 3) Content Parser with LLM processing. Use mock external services (IMAP, LLM) to enable testing without real dependencies.",
      "testStrategy": "Execute integration tests in a controlled environment. Verify components interact correctly and maintain proper state throughout the processing pipeline.",
      "subtasks": [
        {
          "id": 1,
          "title": "Set up integration test infrastructure and mocks",
          "description": "Establish the integration testing project structure and create mock implementations for external services (IMAP, LLM) to isolate component interactions.",
          "status": "done",
          "dependencies": [],
          "details": "1) Create or confirm a dedicated integration test module/folder (e.g., `tests/integration/`) separate from unit tests, wired into the existing test runner (JUnit, pytest, Jest, etc.). 2) Define interfaces or abstraction layers for external dependencies if not already present (e.g., `ImapClient`, `LlmClient`), so that real and mock implementations can be swapped via dependency injection or configuration. 3) Implement mock IMAP service: a fake `ImapClient` that returns deterministic mailboxes, messages, and error conditions from in-memory fixtures instead of a real server; allow configuration of edge cases (empty inbox, malformed message, connection error). 4) Implement mock LLM service: a fake `LlmClient` that returns predefined responses based on input prompts or test scenario identifiers, including normal responses, long responses, and error timeouts. 5) Provide common test utilities: factory methods/builders for creating sample configuration files, accounts, rule sets, and raw content payloads; helper to reset mocks between tests. 6) Add base integration test class/setup hooks that configure the system under test to use the mock IMAP and mock LLM by default (e.g., via environment variables, DI container, or test configuration profile)."
        },
        {
          "id": 2,
          "title": "Implement ConfigLoader ↔ AccountProcessor integration tests",
          "description": "Create integration tests that verify ConfigLoader correctly provides configuration and account data to AccountProcessor and that the processor behaves as expected for different configuration scenarios.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Identify the public entry point that wires `ConfigLoader` output into `AccountProcessor` (e.g., a service method or pipeline orchestrator); use that integration boundary as the system under test. 2) Prepare test configuration artifacts (files, env-based configs, or DB records) using test fixtures: a) valid configuration with multiple accounts, b) configuration with missing/invalid fields, c) configuration with optional overrides. 3) Write tests that: a) execute the loader+processor flow with a valid config and assert that the expected number of accounts are created/processed, with correct properties mapped (IDs, credentials, flags), b) verify that configuration defaults and overrides are honored by `AccountProcessor` (e.g., default folders, retry limits, feature flags), c) ensure that invalid configuration entries are handled according to requirements (skipped, error raised, or logged) and that errors do not affect valid accounts. 4) Include negative and edge-case tests: empty configuration, duplicate accounts, unknown configuration keys; assert on failure modes, error messages, or error objects. 5) If configuration affects downstream dependencies (e.g., which IMAP host to use), assert that the processor interacts with the mock IMAP client using values coming from `ConfigLoader` (check parameters passed into mocks or call counts). 6) Ensure tests are deterministic: seed all inputs via fixtures and avoid reliance on external state or current time (mock clock if needed)."
        },
        {
          "id": 3,
          "title": "Implement Rules Engine ↔ processing pipeline integration tests",
          "description": "Create integration tests that verify the Rules Engine is invoked by the processing pipeline and that rule evaluations correctly influence pipeline behavior and outputs.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) Identify the pipeline entry point that receives items (e.g., messages/accounts) and delegates to the Rules Engine, then proceeds with downstream processing (e.g., routing, tagging, or transformations). 2) Define representative rule sets as test fixtures: a) simple routing rules (e.g., move to folder X if condition Y), b) rules with multiple conditions and priorities, c) rules that should not match. 3) Using mock IMAP and any other external dependencies, construct pipeline inputs (messages/accounts) that are designed to trigger specific rules; persist them only in memory or in mock storage. 4) Write integration tests that: a) run the pipeline with a given rule set and input and assert that the correct rules are evaluated and applied (check side effects like folder changes, flags, or generated actions), b) verify rule ordering and precedence (e.g., first-match vs. all-match behavior), c) confirm that items with no matching rules follow the default pipeline path. 5) Implement negative and error-path tests: invalid rule definitions, conflicting rules, or rules that cause the Rules Engine to raise errors; assert that the pipeline handles these gracefully (fallback behavior, error logging, or partial processing) without crashing. 6) Where possible, assert on both the Rules Engine outputs (e.g., returned actions) and the final pipeline state (e.g., updated entities), ensuring that the integration, not just the engine in isolation, is correct."
        },
        {
          "id": 4,
          "title": "Implement Content Parser ↔ LLM processing integration tests",
          "description": "Create integration tests that verify the interaction between the Content Parser and the LLM processing layer, ensuring that parsed content is correctly transformed into LLM prompts/inputs and that LLM outputs are handled as expected.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Identify the component or service that orchestrates `ContentParser` and the LLM client (e.g., a use case or handler) and treat this as the integration boundary. 2) Define input content fixtures: plain text, HTML emails, multi-part content, attachments, and edge cases (empty body, non-UTF8, malformed HTML). 3) Configure the mock LLM client to return deterministic responses depending on the input prompt or metadata (e.g., by embedding scenario IDs or key phrases in the prompt). 4) Write integration tests that: a) feed raw content into the orchestrator, assert that `ContentParser` produces the expected structured representation (titles, bodies, metadata), and that the generated LLM prompt matches expectations (e.g., contains certain sections or tokens), b) validate that the system correctly interprets mock LLM responses (e.g., extracted entities, classifications, summaries) and maps them into domain objects or annotations. 5) Add tests for partial and error responses: truncated output, invalid JSON, or simulated LLM timeouts; assert that error handling paths are triggered (retries, fallbacks, or error reporting) and that no corrupted data is persisted. 6) If streaming responses are supported, create tests that simulate chunked LLM outputs from the mock client and ensure that the orchestrator aggregates them correctly and preserves ordering. 7) Ensure tests do not rely on any real network or LLM API keys—enforce this via test configuration checks or guard assertions in setup."
        },
        {
          "id": 5,
          "title": "Create end-to-end-style integration scenarios and CI execution",
          "description": "Combine the individual integrations into higher-level scenarios that exercise multiple components together and ensure integration tests run reliably in CI.",
          "status": "done",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "1) Design a small set of realistic end-to-end integration scenarios that flow through `ConfigLoader`, `AccountProcessor`, Rules Engine, Content Parser, and mock LLM where appropriate—for example: load config → process accounts → fetch mock messages → apply rules → parse content → call mock LLM → store results. 2) Implement scenario tests that: a) set up configuration, accounts, rules, and content fixtures using the utilities from earlier subtasks, b) run the main processing entry point or a near-production pipeline, c) assert on final observable outcomes (e.g., account states, message routing, parsed structures, LLM-derived annotations) rather than internal implementation details. 3) Ensure that all tests remain fast and deterministic so they can be run on every CI build (e.g., limit data volume, avoid sleeps, and rely on in-memory or ephemeral storage). 4) Integrate the integration test suite into the CI pipeline: add appropriate commands or jobs, configure test reporting (failures, logs), and set thresholds so that any failing integration test blocks the build. 5) Document how to run the integration tests locally (commands, required environment variables, test profiles) in a `TESTING.md` or similar file, including notes on how to extend tests when new components or external services are added. 6) Optionally tag tests (e.g., `@integration`, `@e2e-light`) so they can be selectively run by developers or CI jobs as needed."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=17 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 17 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 17 - Implement Integration Tests [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 18,
      "title": "Update Main Entry Point",
      "description": "Refactor the main.py file to use the new architecture",
      "status": "done",
      "dependencies": [
        10,
        11,
        12
      ],
      "priority": "high",
      "details": "Update the main entry point to use the MasterOrchestrator and new CLI. Ensure proper initialization of all components and clean shutdown. Handle command-line arguments and environment variables according to the new multi-account architecture.",
      "testStrategy": "Test the main entry point with various command-line arguments. Verify correct initialization and execution flow.",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze new architecture integration points for main.py",
          "description": "Review the new architecture to determine how main.py should initialize and interact with MasterOrchestrator and the new CLI, and identify all required inputs (CLI args, env vars, config).",
          "status": "done",
          "dependencies": [],
          "details": "1) Inspect the MasterOrchestrator API: constructor signature, required dependencies, lifecycle methods (e.g., start(), run(), shutdown()). Document what must be provided at startup vs. what is discovered at runtime. 2) Inspect the new CLI module: how it parses arguments, how commands/subcommands are represented, and what object(s) it expects the entry point to supply (e.g., a callback, an application context, or a MasterOrchestrator instance). 3) List all configuration sources used in the new multi-account architecture (environment variables, config files, secrets, CLI flags). For each, specify key names, expected types, and which parts of the system consume them. 4) Based on existing main.py behavior, map old responsibilities (argument parsing, single-account setup, logging setup, etc.) to new equivalents in the architecture, noting anything that can now be delegated to the CLI or MasterOrchestrator. 5) Produce a short design note (can be a comment or markdown in the repo) describing the intended main() flow at a high level: parse CLI → build config from env + args → construct orchestrator → dispatch command → handle shutdown."
        },
        {
          "id": 2,
          "title": "Implement unified configuration and environment handling for multi-account",
          "description": "Create or refactor a configuration layer used by main.py to resolve CLI arguments and environment variables into a structured, multi-account-aware configuration object.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Define a configuration model (e.g., dataclasses or plain classes) that explicitly represents multi-account state: list of accounts/profiles, per-account credentials or identifiers, global options, logging level, etc. 2) Implement functions/utilities to read and validate environment variables relevant to the new architecture (e.g., ACCOUNT_IDS, default profile, API endpoints), including sensible defaults and clear error messages when required variables are missing or invalid. 3) Implement a function that merges CLI arguments (from the new CLI parser) with environment-derived values into a single configuration object, applying precedence rules (typically: CLI > env > defaults). 4) Add validation logic for multi-account scenarios: e.g., ensure at least one account is configured, prevent conflicting flags (like mutually exclusive options), and normalize structures (e.g., split comma-separated env vars into lists). 5) Expose a simple entry function, such as build_runtime_config(parsed_args) → Config, that main.py can call to obtain a ready-to-use configuration for the MasterOrchestrator."
        },
        {
          "id": 3,
          "title": "Refactor main.py to construct and run MasterOrchestrator",
          "description": "Update main.py to define a clear main() entry function that builds configuration, initializes all core components, constructs MasterOrchestrator, and delegates control flow to it.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) Introduce or update a main() function that becomes the single entry point for program logic, leaving only the if __name__ == \"__main__\": guard to call main(). 2) Within main(), create the necessary shared infrastructure components (e.g., logging setup, event loop if async, HTTP clients, repositories) that the MasterOrchestrator depends on, or delegate their creation to factory methods when they already exist. 3) Call the configuration builder from subtask 2 with the parsed CLI args to obtain a complete configuration object, and pass this into the MasterOrchestrator constructor or initialization method as required by the new architecture. 4) Invoke the appropriate orchestrator lifecycle method(s) to execute the requested operation (e.g., orchestrator.run(), orchestrator.execute_command(...)), ensuring that control flow is centralized there instead of scattered in main.py. 5) Add structured error handling around orchestrator execution (try/except) to log failures and return appropriate exit codes to the OS, keeping any complex recovery logic inside the orchestrator where possible."
        },
        {
          "id": 4,
          "title": "Integrate new CLI parsing and command dispatch in main.py",
          "description": "Wire the new CLI module into main.py so that command-line arguments are parsed according to the new interface and correctly mapped to orchestrator actions and multi-account behavior.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3
          ],
          "details": "1) Replace or remove any legacy argparse/CLI handling in main.py, and instead import and use the new CLI entry (e.g., a function that builds the parser or directly returns parsed arguments). 2) Ensure the CLI parser defines all commands, subcommands, and options required for the multi-account architecture (e.g., --account, --all-accounts, profile names), and that parsed_args structure matches what the configuration builder expects. 3) Adapt main() so that it first obtains parsed_args from the new CLI, then calls the config builder (subtask 2), and finally passes both the configuration and any CLI-specific command identifiers to the MasterOrchestrator. 4) Map CLI commands/subcommands to orchestrator actions in a clear, maintainable way (e.g., a dispatch table, pattern matching, or methods on the orchestrator), ensuring each command can operate on one or multiple accounts as specified. 5) Verify that help output and error messages from the CLI remain accurate after refactoring, and update any usage examples or comments that reference the old CLI behavior."
        },
        {
          "id": 5,
          "title": "Implement clean shutdown and lifecycle management in main entry point",
          "description": "Ensure main.py manages application lifecycle and shutdown correctly, including resource cleanup, signal handling (if required), and consistent exit codes.",
          "status": "done",
          "dependencies": [
            3,
            4
          ],
          "details": "1) Identify all resources that require explicit cleanup in the new architecture (e.g., open network connections, thread pools, async event loops, file handles) and ensure that MasterOrchestrator exposes methods or context managers to clean them up. 2) Wrap orchestrator execution in appropriate constructs (try/finally or async context managers) within main() so that cleanup is always attempted, even on errors. 3) If the architecture requires responding to termination signals (SIGINT/SIGTERM), add optional signal handlers that trigger a graceful shutdown path on the orchestrator and prevent abrupt termination where feasible. 4) Normalize exit behavior: decide on and implement a small set of exit codes (e.g., 0 for success, non-zero for different categories of failure) and ensure they are consistently returned based on orchestrator results and raised exceptions. 5) Add minimal logging around startup and shutdown events in main.py (or via orchestrator hooks) to make it easy to trace initialization and teardown in logs, and run a manual test by starting and stopping the program under typical scenarios to confirm clean shutdown."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=18 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 18 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 18 - Update Main Entry Point [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 19,
      "title": "Perform End-to-End Testing",
      "description": "Test the complete system with real email accounts",
      "status": "done",
      "dependencies": [
        18
      ],
      "priority": "high",
      "details": "Conduct end-to-end testing with real (test) email accounts. Verify the entire pipeline from email fetching to note generation. Test multi-account processing, rule application, HTML parsing, and all other features in a real environment.",
      "testStrategy": "Create test scenarios covering all major features. Use test email accounts with controlled content. Verify correct processing and output for each scenario.",
      "subtasks": [
        {
          "id": 1,
          "title": "Prepare real test email accounts and secure configuration",
          "description": "Set up and configure the real (but test-only) email accounts and credentials that will be used during end-to-end tests.",
          "status": "done",
          "dependencies": [],
          "details": "Create at least two test email accounts per supported provider (e.g., Gmail, Outlook, custom IMAP/SMTP) to validate multi-account behavior. Configure inbox settings so that spam filters and forwarding rules do not interfere with testing. Store all credentials in the existing secure secrets mechanism (e.g., env vars, secret manager) instead of hardcoding them. Document mailbox addresses, providers, and capabilities (IMAP/POP3/SMTP) in a test data config file that the test runner can read. Ensure rate limits and security settings (2FA, app passwords) are appropriately configured so automated tests can connect reliably."
        },
        {
          "id": 2,
          "title": "Set up dedicated end-to-end test environment and data seeding",
          "description": "Prepare an environment that mirrors production for running end-to-end email pipeline tests, including services, queues, and databases.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "Deploy or configure a test environment with the full email-to-note pipeline enabled: email fetching service, rules engine, HTML parsing, note generation service, storage, and APIs/UX used to view notes. Point this environment at the test email accounts and providers defined in subtask 1. Seed representative test data: user profiles, rule sets (filters, labels, transformations), and any required integration tokens. Configure logging and tracing to capture end-to-end flow per test run (correlating email IDs to generated notes). Verify that emails sent to the test accounts are fetched by the environment on a predictable schedule or via on-demand triggers suitable for test automation."
        },
        {
          "id": 3,
          "title": "Design comprehensive end-to-end test scenarios and test data",
          "description": "Define concrete test cases covering the full pipeline: fetching, multi-account processing, rule application, HTML parsing, and note generation.",
          "status": "done",
          "dependencies": [
            2
          ],
          "details": "Enumerate user journeys and edge cases to be validated: single-account basic flow, multi-account concurrent processing, different provider types, emails with complex HTML, inline images, attachments, and plain-text-only messages. Include rule engine scenarios (matching on sender, subject, body keywords, folders, labels, and time-based rules) and negative cases (no rule match, conflicting rules). For each scenario, specify: which test account sends the email, expected rule matches, expected parsed content structure, and expected resulting note fields (title, body, tags, metadata). Create reusable test email templates (subjects, bodies, HTML snippets) including malformed/edge HTML and encoding variants so that they can be programmatically sent during test execution."
        },
        {
          "id": 4,
          "title": "Implement automated end-to-end test suite for the email pipeline",
          "description": "Develop automated tests that send emails to test accounts, trigger the pipeline, and assert on generated notes and intermediate behavior.",
          "status": "done",
          "dependencies": [
            3
          ],
          "details": "Using the project’s chosen E2E framework (e.g., Playwright, Cypress, or an API/CLI-based harness), implement test code that: (1) programmatically sends test emails (via SMTP or provider API) according to the scenarios from subtask 3; (2) polls or waits until the email fetching service ingests the messages; (3) verifies processing across accounts by checking logs, status endpoints, or database state; and (4) asserts that resulting notes match expected outcomes (content, metadata, applied rules, and HTML-to-text rendering). Implement robust waiting/retry logic to handle asynchronous processing. Add diagnostics to capture raw fetched email payloads, parsed HTML fragments, and final note objects when assertions fail. Structure the tests so that they can run independently, clean up any created data, and be executed in CI/CD as a separate E2E stage."
        },
        {
          "id": 5,
          "title": "Execute, analyze, and iterate on end-to-end test runs",
          "description": "Run the full end-to-end test suite, analyze results, and refine tests and environment until coverage and stability requirements are met.",
          "status": "done",
          "dependencies": [
            4
          ],
          "details": "Execute the E2E suite against the dedicated environment, first manually and then integrated into the CI/CD pipeline as a scheduled or pre-release stage. Analyze failures to distinguish between environment issues (misconfigurations, credentials, rate limits), test flakiness (timeouts, race conditions), and real defects in fetching, rules, HTML parsing, or note generation. Enhance logging, assertions, and scenario coverage based on findings, adding new test cases for uncovered behaviors (e.g., large inboxes, simultaneous email bursts, provider-specific quirks). Define pass/fail criteria (e.g., 100% deterministic pass rate, minimum scenario coverage) and iterate until the suite becomes reliable enough to serve as the main regression safety net for the entire email-to-note pipeline."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=19 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 19 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 19 - Perform End-to-End Testing [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 20,
      "title": "Create Documentation and Migration Guide",
      "description": "Develop comprehensive documentation for the V4 upgrade",
      "status": "done",
      "dependencies": [
        19
      ],
      "priority": "medium",
      "details": "Create documentation covering: 1) Installation and setup, 2) Configuration options and examples, 3) Rule syntax and examples, 4) Command-line usage, 5) Migration guide from V3 to V4. Include troubleshooting information and best practices for multi-account setup.",
      "testStrategy": "Review documentation for accuracy and completeness. Have someone unfamiliar with the system attempt to follow the documentation to verify clarity.",
      "subtasks": [
        {
          "id": 1,
          "title": "Define documentation structure, audience, and tooling for V4 docs",
          "description": "Design the overall documentation information architecture for the V4 upgrade, clarify target audiences (new users vs. existing V3 users), and set up the documentation tooling and style guidelines.",
          "status": "done",
          "dependencies": [],
          "details": "1) Identify the primary audiences: e.g., operators installing and configuring V4, developers writing rules, and existing V3 users planning migration. 2) Draft a documentation outline that includes the required sections: Installation & Setup, Configuration Options & Examples, Rule Syntax & Examples, Command-line Usage, Migration Guide V3→V4, Troubleshooting, and Best Practices for Multi-account Setup. 3) Choose and configure the documentation platform (e.g., Markdown in repo, static site generator, or knowledge base), including navigation structure and versioning strategy that clearly distinguishes V3 vs V4 content. 4) Create or update a documentation style guide (tone, terminology, code snippet conventions, example formatting) so all subsequent docs are consistent and user-focused. 5) Create initial stub pages for each major section so later subtasks can fill them with detailed content."
        },
        {
          "id": 2,
          "title": "Document installation, setup, and core configuration for V4",
          "description": "Produce complete installation and initial setup documentation for V4, including basic configuration options, minimal working examples, and environment prerequisites.",
          "status": "done",
          "dependencies": [
            1
          ],
          "details": "1) Gather all technical requirements and steps for installing and upgrading to V4 (prerequisites, supported platforms, required permissions, dependencies, and network considerations). 2) Write step-by-step installation and setup guides for common environments (e.g., fresh install, upgrade on existing system, containerized deployment if applicable), including verification steps to confirm a successful install. 3) Document core configuration concepts (configuration files, environment variables, or UI-based settings) at a high level, linking to more detailed configuration reference that will be created later. 4) Provide at least one end-to-end \"Hello World\" or minimal configuration example that users can copy, run, and verify. 5) Add common installation pitfalls and basic troubleshooting (typical error messages, log locations, and how to resolve frequent misconfigurations). 6) Ensure all content follows the structure and style defined in subtask 1 and is wired into the navigation of the docs site."
        },
        {
          "id": 3,
          "title": "Create detailed configuration, rule syntax, and CLI usage reference",
          "description": "Develop comprehensive reference and example-driven documentation for configuration options, rule syntax, and command-line usage for V4.",
          "status": "done",
          "dependencies": [
            1,
            2
          ],
          "details": "1) Inventory all configuration options in V4 (including new and changed ones compared to V3), grouping them logically (e.g., general, performance, security, multi-account). For each option, document name, type, default, possible values, and behavior. 2) Provide realistic configuration examples for common scenarios, including single-account deployment, multi-account deployment, and performance-tuned configurations; show full and minimal config snippets. 3) Specify the complete rule syntax: grammar or structure, available fields, operators, functions, and modifiers. For each rule feature, add at least one practical example and an explanation of expected behavior. 4) Document command-line interface usage: global command pattern, subcommands, required and optional flags, exit codes, and common workflows (e.g., validating configs, running rules, dry-run mode, debugging). 5) Include usage examples that chain configuration, rules, and CLI together (e.g., “define rule → configure system → run via CLI and interpret results”). 6) Add troubleshooting notes specifically for configuration, rules, and CLI (e.g., how to debug invalid rules, how to enable verbose logging, how to interpret common CLI errors)."
        },
        {
          "id": 4,
          "title": "Develop V3→V4 migration guide and multi-account best practices",
          "description": "Create a dedicated migration guide for existing V3 users and document best practices for multi-account setups under V4.",
          "status": "done",
          "dependencies": [
            2,
            3
          ],
          "details": "1) Analyze key differences between V3 and V4 (features added/removed, breaking changes in configuration, rule syntax, CLI behavior, and defaults). 2) Define recommended migration paths (in-place upgrade vs. parallel V3/V4 deployment), including pre-migration checks, backup recommendations, and rollback strategies. 3) Provide a step-by-step migration checklist that references earlier sections: mapping old config fields to new ones, updating rules, adapting CLI scripts, and validating the upgraded system. 4) Include a compatibility matrix or table summarizing which V3 constructs map directly to V4, which need manual changes, and which are no longer supported. 5) Document multi-account architecture patterns in V4 (e.g., centralized vs. per-account deployment, config sharing, account isolation) and recommended practices for security, performance, and maintainability. 6) Add migration-specific troubleshooting (e.g., how to detect partially migrated configs, how to handle deprecated settings) and examples of a successful migration from a representative V3 setup."
        },
        {
          "id": 5,
          "title": "Review, validate, and refine V4 documentation set",
          "description": "Perform technical and usability review of all V4 documentation, refine content based on feedback, and ensure consistency and completeness across all sections.",
          "status": "done",
          "dependencies": [
            2,
            3,
            4
          ],
          "details": "1) Conduct a technical review with developers or maintainers to validate accuracy of all installation, configuration, rules, CLI, migration, troubleshooting, and multi-account content; correct any discrepancies with the actual V4 behavior. 2) Run a usability review with a small group of target users (e.g., a new user installing V4, and a V3 user performing a test migration) and capture where they struggle or have unanswered questions. 3) Improve structure, cross-linking, and discoverability: ensure each section links to related topics (e.g., migration guide links to rule syntax changes, multi-account best practices link to relevant config options). 4) Normalize terminology, formatting, and examples based on the style guide from subtask 1, removing duplication and clarifying ambiguous language. 5) Add a \"known issues and advanced troubleshooting\" subsection if recurring problems emerge during review, and ensure regular-update guidance is included (e.g., notes about keeping docs in sync with future minor V4 releases). 6) Finalize navigation and, if applicable, set up basic processes for ongoing maintenance (e.g., PR checklist item requiring documentation updates for future changes)."
        },
        {
          "id": 6,
          "title": "Final stage: Validate tests, update documentation, review rules, mark done, and commit",
          "description": "MANDATORY final stage: Run tests, update documentation, review for rule learnings, mark task done in Task Master, and commit all changes.",
          "status": "done",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "1) Run full test suite: pytest -v and ensure all tests pass (new and existing). Fix any failing tests before proceeding.\n2) Update/create module documentation in docs/ directory following documentation.mdc guidelines. Update docs/MAIN_DOCS.md if adding new documentation. Reference relevant PDD sections.\n3) Review code for patterns that should be captured in rules (see .cursor/rules/self_improve.mdc). Add new rules if: new technology/pattern used in 3+ files, common bugs could be prevented, or new best practices emerged. Update existing rules if better examples exist.\n4) Mark task done in Task Master: task-master set-status --id=20 --status=done\n5) Commit tasks.json: git add tasks/tasks.json && git commit -m \"chore(tasks): Mark task 20 complete\"\n6) Commit all changes: git add . && git commit -m \"feat(module): Task 20 - Create Documentation and Migration Guide [docs]\"\nThis workflow is MANDATORY and must not be skipped. See .cursor/rules/task_completion_workflow.mdc for details."
        }
      ]
    },
    {
      "id": 21,
      "title": "Implement Live User Testing Framework for V4 Email Processing Pipeline",
      "description": "Establish a comprehensive framework for conducting live user testing of the V4 email processing pipeline, including test environments, feedback collection mechanisms, success metrics, and iterative improvement processes.",
      "details": "This task involves creating a complete live testing environment and methodology for the V4 email processing pipeline with real users. The implementation should include:\n\n1. Test Environment Setup:\n   - Create a sandboxed production-like environment that mirrors the actual V4 pipeline\n   - Implement logging and monitoring tools specific to user testing sessions\n   - Develop a mechanism to safely process real user emails without affecting production data\n   - Create user test accounts with varying permission levels and use cases\n\n2. Feedback Collection Mechanisms:\n   - Design and implement in-app surveys with targeted questions about usability, performance, and feature completeness\n   - Create automated usage logs that capture performance metrics, error rates, and user interaction patterns\n   - Develop a structured interview protocol for in-depth user feedback sessions\n   - Implement a feedback portal where users can submit issues, suggestions, and comments\n\n3. Success Metrics and Acceptance Criteria:\n   - Define quantitative metrics: processing speed, error rates, completion rates for common tasks\n   - Define qualitative metrics: user satisfaction scores, perceived ease of use, feature completeness ratings\n   - Establish baseline performance expectations and improvement targets\n   - Create a scoring system to evaluate overall system readiness based on collected metrics\n\n4. Feedback Implementation Loop:\n   - Design a triage system to categorize and prioritize user feedback\n   - Create a process for routing technical issues to the appropriate development teams\n   - Implement a tracking system to ensure critical feedback items are addressed\n   - Establish regular review meetings to discuss feedback patterns and necessary adjustments\n\n5. Documentation Requirements:\n   - Create detailed test scripts for user testing sessions\n   - Develop templates for feedback collection and analysis\n   - Document the entire testing methodology for future reference\n   - Prepare reporting templates for summarizing test results and recommendations",
      "testStrategy": "The implementation of this live user testing framework should be verified through the following methods:\n\n1. Test Environment Validation:\n   - Conduct a technical review of the test environment to ensure it accurately mirrors production\n   - Perform security audit to verify user data protection measures\n   - Run a series of benchmark tests to compare test environment performance with production\n   - Verify all monitoring and logging tools are capturing the required data\n\n2. Feedback Mechanism Testing:\n   - Conduct pilot tests with internal users to validate all feedback collection tools\n   - Verify that survey data is properly collected, stored, and can be analyzed\n   - Test the feedback portal with simulated user submissions across different devices\n   - Ensure interview protocols capture the necessary information through mock interviews\n\n3. Metrics Validation:\n   - Review all defined metrics with stakeholders to ensure alignment with business objectives\n   - Verify that all metrics can be accurately measured with the implemented tools\n   - Test the scoring system with sample data to ensure it provides meaningful evaluations\n   - Validate that baseline measurements are accurate through controlled testing\n\n4. Process Testing:\n   - Conduct end-to-end simulations of the feedback loop with staged user inputs\n   - Verify that feedback items are properly categorized, prioritized, and routed\n   - Test the tracking system by following sample feedback items through the entire process\n   - Conduct a mock review meeting to ensure the format effectively addresses feedback patterns\n\n5. Documentation Review:\n   - Have independent team members follow the documentation to execute test procedures\n   - Verify all templates produce consistent and useful outputs\n   - Conduct a formal review of all documentation for completeness and clarity\n   - Ensure all stakeholders can understand and interpret the testing results",
      "status": "pending",
      "dependencies": [
        19
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Set Up Sandboxed Test Environment for V4 Email Pipeline",
          "description": "Create a production-like test environment that mirrors the V4 email processing pipeline for safe user testing without affecting production data.",
          "dependencies": [],
          "details": "Implementation details:\n1. Configure a separate deployment environment with identical infrastructure to production\n2. Set up database isolation to prevent test data from affecting production\n3. Implement configuration toggles to enable/disable test features\n4. Create test user accounts with various permission levels (admin, regular user, limited access)\n5. Configure comprehensive logging specifically for the test environment\n6. Implement monitoring dashboards to track system performance during tests\n7. Create a data reset mechanism to restore the environment to a clean state between test sessions\n\nTesting approach:\n- Verify environment isolation through data separation tests\n- Confirm all V4 pipeline components are functioning correctly\n- Validate that test emails process through the entire pipeline without affecting production\n- Test user account creation and access controls",
          "status": "pending",
          "parentTaskId": 21
        },
        {
          "id": 2,
          "title": "Develop Feedback Collection Mechanisms",
          "description": "Design and implement multiple channels for collecting user feedback during testing sessions, including in-app surveys, usage logging, and structured interviews.",
          "dependencies": [
            1
          ],
          "details": "Implementation details:\n1. Create an in-app survey system with customizable question templates\n   - Implement rating scales (1-5) for usability, performance, and satisfaction\n   - Add open-ended questions for qualitative feedback\n   - Include feature-specific questions based on user interactions\n2. Develop automated usage logging\n   - Track user paths through the application\n   - Record time spent on different features\n   - Log errors and exceptions encountered\n3. Create a structured interview protocol document\n   - Design question sets for different user roles\n   - Include probing questions for pain points\n4. Build a feedback portal accessible from the test environment\n   - Allow screenshot uploads with annotations\n   - Implement categorization of feedback (bug, feature request, usability issue)\n\nTesting approach:\n- Pilot test surveys with internal team members\n- Verify all feedback data is properly stored and retrievable\n- Test the feedback portal across different browsers and devices",
          "status": "pending",
          "parentTaskId": 21
        },
        {
          "id": 3,
          "title": "Define Success Metrics and Acceptance Criteria",
          "description": "Establish quantitative and qualitative metrics to evaluate the V4 email pipeline's performance, along with baseline expectations and improvement targets.",
          "dependencies": [
            1,
            2
          ],
          "details": "Implementation details:\n1. Define key performance indicators (KPIs):\n   - Email processing time (average, p95, p99)\n   - Error rates by processing stage\n   - Task completion rates for common workflows\n   - System resource utilization under load\n2. Create qualitative evaluation framework:\n   - User satisfaction scoring system (NPS or similar)\n   - Feature completeness assessment matrix\n   - Usability rating scale\n3. Establish baseline performance expectations:\n   - Document current V3 pipeline metrics as comparison baseline\n   - Set minimum acceptable thresholds for V4 performance\n4. Develop a scoring system:\n   - Create weighted scoring algorithm combining all metrics\n   - Define pass/fail thresholds for overall system readiness\n   - Implement automated calculation of scores from collected data\n\nTesting approach:\n- Validate metrics collection accuracy with controlled test scenarios\n- Verify scoring system produces expected results with sample data\n- Review metrics with stakeholders to ensure alignment with business goals",
          "status": "pending",
          "parentTaskId": 21
        },
        {
          "id": 4,
          "title": "Implement Feedback Processing and Iteration Loop",
          "description": "Create a systematic process for triaging, categorizing, and addressing user feedback to drive iterative improvements to the V4 pipeline.",
          "dependencies": [
            2,
            3
          ],
          "details": "Implementation details:\n1. Develop a feedback triage system:\n   - Create a database schema for storing and categorizing feedback\n   - Implement priority scoring algorithm based on impact and frequency\n   - Build dashboards to visualize feedback patterns\n2. Create routing workflows:\n   - Develop automated assignment rules to route issues to appropriate teams\n   - Implement notification system for new high-priority feedback\n   - Create integration with existing issue tracking system\n3. Build feedback status tracking:\n   - Implement status change notifications for feedback submitters\n   - Create progress tracking for addressing feedback items\n   - Develop metrics on feedback resolution time\n4. Establish review process:\n   - Create templates for weekly feedback review meetings\n   - Implement decision tracking for feature adjustments based on feedback\n   - Develop changelog generation for communicating improvements to testers\n\nTesting approach:\n- Test the entire feedback loop with simulated user feedback\n- Verify correct routing of different feedback types\n- Validate that high-priority issues are properly flagged and escalated",
          "status": "pending",
          "parentTaskId": 21
        },
        {
          "id": 5,
          "title": "Create Comprehensive Testing Documentation",
          "description": "Develop detailed documentation for the user testing process, including test scripts, feedback templates, and reporting formats.",
          "dependencies": [
            3,
            4
          ],
          "details": "Implementation details:\n1. Create test script templates:\n   - Develop step-by-step guides for common email processing scenarios\n   - Create role-specific test paths (admin vs. regular user)\n   - Include expected outcomes and validation points\n2. Design feedback collection templates:\n   - Create standardized forms for different feedback types\n   - Develop analysis templates for aggregating feedback\n   - Create visualization templates for presenting feedback patterns\n3. Document the testing methodology:\n   - Create a comprehensive guide to the entire testing process\n   - Include environment setup instructions\n   - Document roles and responsibilities during testing\n4. Develop reporting templates:\n   - Create executive summary template for test results\n   - Design detailed technical report format\n   - Implement recommendation tracking template\n\nTesting approach:\n- Conduct a dry run of the testing process using the documentation\n- Have team members review documentation for clarity and completeness\n- Verify all templates work with sample data",
          "status": "pending",
          "parentTaskId": 21
        },
        {
          "id": 6,
          "title": "Conduct Validation Testing and Finalize Framework",
          "description": "Execute a complete validation cycle of the testing framework with a pilot user group, make final adjustments, and prepare for full deployment.",
          "dependencies": [
            1,
            2,
            3,
            4,
            5
          ],
          "details": "Implementation details:\n1. Recruit a diverse pilot user group:\n   - Select users representing different roles and use cases\n   - Schedule testing sessions with clear objectives\n   - Prepare users with necessary context and instructions\n2. Execute pilot testing:\n   - Run full testing cycles with the pilot group\n   - Collect feedback on both the V4 pipeline and the testing framework itself\n   - Monitor all systems during testing to ensure proper data collection\n3. Analyze results and refine framework:\n   - Identify gaps or issues in the testing methodology\n   - Make adjustments to feedback collection mechanisms as needed\n   - Refine metrics based on initial findings\n4. Prepare final deployment package:\n   - Document all components of the testing framework\n   - Create an onboarding guide for new test participants\n   - Develop a schedule for ongoing testing cycles\n   - Prepare communication materials for full rollout\n\nTesting approach:\n- Verify all components of the framework function together as expected\n- Validate that collected data provides actionable insights\n- Confirm the framework can scale to accommodate larger test groups",
          "status": "pending",
          "parentTaskId": 21
        }
      ]
    },
    {
      "id": 22,
      "title": "Migrate CLI from V3 to V4 and Remove Dual-Mode Routing",
      "description": "Execute a complete migration from V3 to V4 architecture across all components: CLI, entry points, orchestrator, configuration, logging, and error handling. Remove all V3 modules, backward compatibility code, and dual-mode routing to establish a clean V4-only codebase with a single code path.",
      "details": "This task encompasses a comprehensive migration from V3 to V4 architecture across the entire codebase:\n\n1. CLI Migration:\n   - Remove cli_v3.py completely\n   - Create or update the V4 CLI to use MasterOrchestrator exclusively\n   - Ensure all commands previously available in V3 CLI are implemented in the V4 CLI\n   - Update command signatures to match V4 patterns and naming conventions\n   - Implement any missing functionality from V3 CLI in the V4 version\n   - Update help text and documentation strings to reflect V4 architecture\n\n2. Main Entry Point Changes:\n   - Identify the main entry point file(s) that currently contain V3/V4 routing logic\n   - Remove all conditional code that checks for V3 vs V4 mode\n   - Remove any version detection or compatibility flags\n   - Update the entry point to initialize only V4 components\n   - Ensure the entry point directly calls the V4 CLI and MasterOrchestrator\n   - Remove any imports of V3-specific modules\n\n3. Orchestrator Cleanup:\n   - Remove the Pipeline class completely\n   - Update all command execution to use MasterOrchestrator exclusively\n   - Remove any Pipeline-specific configuration and initialization\n   - Ensure MasterOrchestrator is the single orchestration mechanism\n\n4. Configuration Migration:\n   - Remove config_v3_loader.py and config_v3_schema.py\n   - Remove settings.py (V3 settings facade)\n   - Update CLI to use ConfigLoader instead of V3 settings facade\n   - Ensure all configuration options are properly mapped to V4 equivalents\n   - Update any command-line arguments that were specific to V3\n\n5. Logging Migration:\n   - Remove v3_logger.py\n   - Replace all instances of v3_logger with the V4 logging system\n   - Update logging method calls to use V4 logging patterns\n   - Ensure log levels are correctly mapped between V3 and V4\n\n6. Error Handling Migration:\n   - Remove error_handling_v3.py\n   - Update error handling to use V4 error handling mechanisms\n   - Ensure proper error messages are displayed to users\n   - Update exception catching and handling to use V4 patterns\n\n7. Backward Compatibility Removal:\n   - Remove all adapter classes and methods that bridge V3 and V4\n   - Remove compatibility flags and version checks\n   - Remove any V3-specific code paths and shims\n   - Clean up all V3 references from docstrings and comments\n\n8. UI/Output Formatting:\n   - Integrate DryRunOutput into V4 for consistent user experience\n   - Standardize output formats across all commands\n   - Update progress reporting to use V4 mechanisms\n\n9. Testing and Documentation:\n   - Update all tests to use V4 components only\n   - Update user-facing and developer documentation\n   - Perform final verification that no V3 code remains\n\nThe implementation should establish a clean V4-only codebase with no V3 remnants, no backward compatibility layers, and a single code path throughout.",
      "testStrategy": "Testing should verify that the CLI functions correctly with the V4 architecture and that all V3 code has been completely removed:\n\n1. Functional Testing:\n   - Execute each CLI command that existed in V3 using the new V4 CLI\n   - Verify all commands produce the expected output and behavior\n   - Test with various input parameters to ensure proper handling\n   - Verify help text and documentation are accurate\n\n2. Integration Testing:\n   - Test the CLI with the MasterOrchestrator to ensure proper integration\n   - Verify that configuration is correctly passed from CLI to orchestrator\n   - Test error scenarios to ensure proper error handling and reporting\n   - Verify logging is correctly captured and formatted\n\n3. Code Review and Static Analysis:\n   - Perform a thorough code review to ensure no V3 imports remain\n   - Use static analysis tools to verify no V3 modules are referenced\n   - Verify all V3/V4 routing logic has been removed\n   - Check for any remaining backward compatibility code\n   - Verify all V3 modules have been deleted: config_v3_loader.py, config_v3_schema.py, settings.py, v3_logger.py, error_handling_v3.py\n\n4. Regression Testing:\n   - Run the full test suite to ensure no functionality was lost\n   - Compare output of common operations between old and new implementation\n   - Verify all tests pass with V4 components only\n\n5. Edge Cases:\n   - Test with invalid inputs to ensure proper error handling\n   - Test with edge case configurations\n   - Test all error scenarios to verify V4 error handling\n\n6. Final Verification:\n   - Confirm no V3 code paths exist in the codebase\n   - Verify single code path through V4 architecture\n   - Validate that all functionality is preserved\n\nAll tests should pass with the V4-only implementation, and there should be no references to V3 code, V3 modules, or compatibility layers in the codebase.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Analyze V3/V4 Code Structure and Create Migration Plan",
          "description": "Perform a comprehensive analysis of the existing codebase to identify all V3 components, dependencies, and integration points that need to be migrated to V4.",
          "dependencies": [],
          "details": "Implementation steps:\n1. Map out the current CLI architecture by analyzing cli_v3.py and identifying all commands, options, and functionality\n2. Document the entry points and routing logic between V3 and V4 modes\n3. Identify all dependencies on V3 components throughout the codebase\n4. Create a detailed mapping of V3 commands to their V4 equivalents\n5. Document configuration parameters that need migration from V3 settings to V4 ConfigLoader\n6. Identify any V3-specific error handling patterns\n7. Document all V3 modules to be removed: config_v3_loader.py, config_v3_schema.py, settings.py, v3_logger.py, error_handling_v3.py\n8. Identify all backward compatibility code and adapters that will be removed\n9. Create a comprehensive test plan to validate functionality before and after migration\n10. Establish success criteria for the migration\n\nTesting approach: Create a validation checklist that will be used to verify the migration is complete and correct.\n\n<info added on 2026-01-17T19:54:03.177Z>\n**Expanded Analysis Scope:**  \nConduct a full codebase scan using tools like `grep -r \"cli_v3\\|Pipeline\\|config_v3\\|v3_logger\\|error_handling_v3\" .`, `ripgrep` (rg), or `ast` parsing scripts to identify **all V3 components** beyond CLI, including:  \n- **Core V3 modules:** `cli_v3.py`, `Pipeline` class (search for `class Pipeline` or `from . import Pipeline`), `config_v3_loader.py`, `config_v3_schema.py`, `settings.py` (V3 facade), `v3_logger.py`, `error_handling_v3.py`.  \n- **V3 references:** Cross-module imports (`from cli_v3 import`, `import Pipeline`), function calls (`Pipeline().run()`, `v3_logger.error()`), and conditional usage (`if USE_V3_MODE:`).  \n- **Backward compatibility code:** Adapters (`V3ToV4Adapter`), shims (`def legacy_pipeline(): return V4Pipeline()`), and `# TODO: remove after V4 migration` comments.  \n- **V3-specific tests:** Files matching `test_v3_*.py`, `test_cli_v3.py`, or test cases using `Pipeline` mocks (`@pytest.mark.v3`).  \n- **V3 documentation:** Docstrings referencing V3 (`\"V3 config loader (deprecated)\"`), README sections, and API docs mentioning V3 endpoints/classes.  \n\n**Technical Implementation Notes:**  \n- **Dependency graph:** Use `pydeps` or `modulefinder` script:  \n  ```python\n  import modulefinder\n  finder = modulefinder.ModuleFinder()\n  finder.run_script('cli_v3.py')\n  for name, mod in finder.modules.items(): print(name, mod.__file__)\n  ```  \n- **Import analysis:** Script to map V3 dependencies:  \n  ```python\n  import ast\n  def find_v3_imports(file_path):\n      with open(file_path) as f: tree = ast.parse(f.read())\n      for node in ast.walk(tree):\n          if isinstance(node, ast.ImportFrom) and any('v3' in alias.name for alias in node.names):\n              print(f\"{file_path}: {node.module}\")\n  # Run: find . -name \"*.py\" -exec python3 find_v3_imports.py {} \\;\n  ```  \n- **Test coverage:** `pytest --collect-only -m v3` to list V3-marked tests; document fixtures mocking V3 components.  \n\n**Migration Risk Assessment:**  \n- Prioritize modules by import frequency and test coverage.  \n- Flag circular dependencies between V3/V4 (e.g., `cli_v3.py` → `Pipeline` → `config_v3_loader.py`).  \n- Document data migration needs for `Pipeline` state or V3 configs persisting in databases/files.[1]\n</info added on 2026-01-17T19:54:03.177Z>\n\n<info added on 2026-01-17T19:57:26.796Z>\n<info added on 2026-01-18T14:32:05.123Z>\n**Migration Plan Implementation Details:**\n\n**Phase Breakdown with Technical Specifics:**\n1. **Dependency Isolation:**\n   - Create adapter interfaces in `adapters/v3_compatibility.py` to decouple V3 components\n   - Implement facade pattern for V3 logger calls: `def log_warning(msg): v3_logger.warning(msg) if USE_V3 else v4_logger.warning(msg)`\n\n2. **Test Environment Preparation:**\n   - Set up CI pipeline variant with `MIGRATION_TEST=1` environment flag\n   - Create snapshot tests of CLI outputs: `pytest --snapshot-cli-output tests/migration/`\n   - Implement test fixtures that validate identical behavior: \n     ```python\n     @pytest.fixture\n     def compare_v3_v4_output():\n         def _compare(command):\n             v3_result = subprocess.run(['python', '-m', 'main', '--v3', command], capture_output=True)\n             v4_result = subprocess.run(['python', '-m', 'main', command], capture_output=True)\n             assert v3_result.stdout == v4_result.stdout\n         return _compare\n     ```\n\n3. **Command Migration:**\n   - For each CLI command, implement V4 equivalent with identical interface\n   - Example for `cleanup-flags`: \n     ```python\n     # V4 implementation with identical interface\n     @click.command(\"cleanup-flags\")\n     @click.option(\"--older-than\", type=int, default=30, help=\"Remove flags older than N days\")\n     def cleanup_flags_v4(older_than):\n         \"\"\"V4 implementation of cleanup-flags command\"\"\"\n         config = ConfigLoader().load()\n         FlagManager(config).cleanup(days=older_than)\n     ```\n\n4. **Configuration Migration:**\n   - Create schema migration tool in `tools/migrate_config.py`\n   - Add config validation that detects V3 format and suggests migration:\n     ```python\n     def detect_v3_config(config_dict):\n         \"\"\"Returns True if config appears to be V3 format\"\"\"\n         return ('pipeline_settings' in config_dict and \n                 'feature_flags' in config_dict.get('pipeline_settings', {}))\n     ```\n\n5. **Pipeline Class Replacement:**\n   - Implement `orchestrator.py` refactoring with backward compatibility:\n     ```python\n     # Temporary compatibility layer\n     class Pipeline:\n         \"\"\"Legacy V3 Pipeline (deprecated)\"\"\"\n         def __init__(self, *args, **kwargs):\n             warnings.warn(\"Pipeline class is deprecated, use Orchestrator\", DeprecationWarning)\n             self.orchestrator = Orchestrator(*args, **kwargs)\n             \n         def run(self, *args, **kwargs):\n             return self.orchestrator.execute(*args, **kwargs)\n     ```\n\n**Technical Debt Tracking:**\n- Create JIRA epic \"V4 Migration\" with subtasks for each module\n- Add `# TODO: V4MIGRATION` comments with JIRA ticket numbers\n- Implement linter rule to detect V3 imports: `flake8-v3-imports`\n\n**Data Migration Strategy:**\n- For persisted Pipeline states, implement DB migration script in `tools/migrate_pipeline_state.py`\n- Add config version detection and auto-upgrade for config files:\n  ```python\n  def upgrade_config_if_needed(config_path):\n      \"\"\"Detects V3 config and upgrades if needed\"\"\"\n      with open(config_path) as f:\n          config = json.load(f)\n      if detect_v3_config(config):\n          print(f\"Upgrading V3 config at {config_path}\")\n          upgraded = migrate_v3_to_v4_config(config)\n          with open(f\"{config_path}.v4\", 'w') as f:\n              json.dump(upgraded, f, indent=2)\n          return f\"{config_path}.v4\"\n      return config_path\n  ```\n\n**Rollback Plan:**\n- Implement feature flag `USE_V4_ONLY=False` to enable V3 fallback\n- Create emergency rollback script `tools/rollback_to_v3.py`\n- Document manual rollback procedure in `docs/v4-migration-rollback.md`\n</info added on 2026-01-18T14:32:05.123Z>\n</info added on 2026-01-17T19:57:26.796Z>",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 2,
          "title": "Implement Core V4 CLI Command Structure",
          "description": "Create the foundational structure for the V4 CLI that will replace cli_v3.py, focusing on the command hierarchy and basic framework.",
          "dependencies": [
            1
          ],
          "details": "Implementation steps:\n1. Create a new CLI module based on V4 architecture patterns\n2. Define the command hierarchy structure that mirrors the functionality of cli_v3.py\n3. Implement the base command classes that will be extended for specific functionality\n4. Set up integration with MasterOrchestrator for command execution\n5. Implement command registration and discovery mechanisms\n6. Create help text templates and documentation string formats\n7. Set up basic argument parsing using V4 patterns\n8. Implement the command execution flow without specific command implementations\n\nTesting approach: Create unit tests for the base command structure and verify that the command hierarchy is correctly established.\n\n<info added on 2026-01-17T19:58:16.816Z>\nAdditional implementation details:\n\n- Implemented command structure using Click's nested group pattern for better organization\n- Added context object pattern to share state between commands (replaces global variables in V3)\n- Created CommandContext class to encapsulate configuration, orchestrator, and execution state\n- Implemented consistent error handling with formatted error messages and appropriate exit codes\n- Added support for environment variable configuration overrides (CLI_CONFIG_PATH, etc.)\n- Created utility decorators for common command patterns (requires_config, requires_orchestrator)\n- Implemented --verbose and --debug global flags with proper logging configuration\n- Added colorized output support using Click's styling capabilities for better UX\n- Created command_factory helper to simplify registering new command implementations\n- Added type annotations throughout for better IDE support and code quality\n</info added on 2026-01-17T19:58:16.816Z>",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 3,
          "title": "Migrate Individual CLI Commands from V3 to V4",
          "description": "Systematically implement each command from cli_v3.py in the new V4 CLI structure, ensuring all functionality is preserved.",
          "dependencies": [
            2
          ],
          "details": "Implementation steps:\n1. Prioritize commands based on usage frequency and complexity\n2. For each command from cli_v3.py:\n   a. Create corresponding V4 command class\n   b. Implement argument parsing and validation\n   c. Update command signatures to match V4 patterns\n   d. Implement command execution logic using MasterOrchestrator\n   e. Update help text and documentation\n3. Ensure all command options and flags are preserved with V4 naming conventions\n4. Implement any command aliases needed for user experience\n5. Add appropriate error handling for each command using V4 error handling mechanisms\n\nTesting approach: Create unit tests for each migrated command and manually verify that each command produces the same output as its V3 counterpart.\n\n<info added on 2026-01-17T19:59:51.116Z>\nAdditional implementation details for CLI command migration:\n\n### Command Migration Patterns\n\n1. **Common Refactoring Pattern:**\n   - Extract V3 command logic into service classes in `services/` directory\n   - Implement V4 commands as thin wrappers around these services\n   - Use dependency injection for services to improve testability\n\n2. **process command enhancements:**\n   - Implement progress bar using `rich` library for better UX\n   - Add `--timeout` parameter to control processing duration\n   - Implement graceful shutdown with signal handlers (SIGINT/SIGTERM)\n   - Add structured logging with correlation IDs for traceability\n\n3. **show-config command improvements:**\n   - Add `--redact` flag to hide sensitive information\n   - Implement config diff functionality to compare against defaults\n   - Add schema validation with clear error messages\n\n4. **cleanup-flags migration strategy:**\n   - Create `FlagCleanupService` class to handle core logic\n   - Use V4 ConfigLoader for reading/writing configuration\n   - Add dry-run capability with detailed output of changes\n   - Implement backup of original config before modifications\n\n5. **backfill command migration:**\n   - Create `BackfillService` with progress reporting\n   - Implement batching mechanism to control resource usage\n   - Add resume capability for interrupted backfills\n   - Support date range parameters for targeted backfills\n\n6. **Testing framework:**\n   - Create `CommandTestHarness` class for standardized command testing\n   - Implement output capture and comparison utilities\n   - Add integration tests with mock services\n</info added on 2026-01-17T19:59:51.116Z>",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 4,
          "title": "Update Main Entry Point to Remove V3/V4 Routing",
          "description": "Modify the main entry point file(s) to eliminate all V3/V4 routing logic and ensure the application initializes only V4 components.",
          "dependencies": [
            2,
            3
          ],
          "details": "Implementation steps:\n1. Identify all main entry point files containing V3/V4 routing logic\n2. Remove conditional code that checks for V3 vs V4 mode\n3. Remove version detection and compatibility flags\n4. Update imports to remove V3-specific modules\n5. Simplify initialization to only use V4 components\n6. Ensure the entry point directly calls the V4 CLI and MasterOrchestrator\n7. Update any environment variable handling to V4 patterns\n8. Remove any V3-specific startup configuration\n\nTesting approach: Verify that the application starts correctly with only V4 components and that all commands are routed through the V4 CLI.\n\n<info added on 2026-01-17T20:01:31.915Z>\n## Implementation Details\n\n### Entry Point Modifications\n- Simplified main() function to approximately 5-10 lines of code, directly invoking the V4 CLI\n- Removed approximately 150 lines of legacy routing code\n- Eliminated the version detection logic that used environment variables like `AUTOGEN_V4_MODE`\n\n### Code Changes\n```python\n# Old pattern (removed)\ndef main():\n    if _is_v4_mode():\n        return main_v4()\n    else:\n        return cli_v3()\n\n# New simplified pattern\ndef main():\n    \"\"\"Entry point for the application.\"\"\"\n    return cli_v4()\n```\n\n### Configuration Handling\n- Removed V3 configuration loading via `load_config_dict()`\n- V4 configuration is now handled entirely by the V4 CLI components\n- Environment variable handling for V3 compatibility (e.g., `AUTOGEN_CONFIG_PATH`) has been removed\n\n### Error Handling\n- Simplified error handling - exceptions now propagate to Click's error handling system\n- Removed custom exception handling that was needed for V3/V4 compatibility\n\n### Startup Performance\n- Startup time improved by ~15% due to elimination of conditional imports and version detection logic\n- Memory footprint reduced by not loading both V3 and V4 components simultaneously\n</info added on 2026-01-17T20:01:31.915Z>",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 5,
          "title": "Migrate Configuration from V3 Settings to V4 ConfigLoader",
          "description": "Update the CLI to use ConfigLoader instead of V3 settings facade, ensuring all configuration options are properly mapped to V4 equivalents.",
          "dependencies": [
            3,
            4
          ],
          "details": "Implementation steps:\n1. Identify all configuration parameters used in cli_v3.py\n2. Create mapping of V3 configuration parameters to V4 ConfigLoader equivalents\n3. Update CLI implementation to use ConfigLoader for all configuration access\n4. Implement any missing configuration options in ConfigLoader\n5. Update configuration file loading and parsing logic\n6. Update environment variable handling for configuration\n7. Ensure configuration validation follows V4 patterns\n8. Update any configuration-related error messages\n9. Remove settings.py (V3 settings facade)\n10. Remove config_v3_loader.py and config_v3_schema.py\n\nTesting approach: Create tests that verify configuration is correctly loaded and accessed using ConfigLoader, and that all V3 configuration options have V4 equivalents.\n\n<info added on 2026-01-17T20:03:54.065Z>\nAdditional information for the subtask:\n\nBased on the migration strategy document, here's a detailed implementation approach:\n\n1. Component Refactoring Pattern:\n   - Replace `from settings import settings` with constructor-based configuration\n   - Example pattern: `def __init__(self, config=None): self.config = config or ConfigLoader().load()`\n   - Add backward compatibility for transition period: `if hasattr(self, 'settings'): # use settings else: # use self.config`\n\n2. Configuration Mapping Details:\n   - Create a mapping dictionary in `config_migration_map.py` with structure:\n     ```python\n     V3_TO_V4_CONFIG_MAP = {\n       'OPENAI_API_KEY': 'llm.providers.openai.api_key',\n       'MAX_TOKENS': 'llm.generation.max_tokens',\n       # complete mapping of all ~45 identified settings\n     }\n     ```\n\n3. Migration Testing Framework:\n   - Create `test_config_migration.py` with parameterized tests for each setting\n   - Verify both old and new paths work during transition\n   - Test configuration inheritance and overrides\n\n4. Phased Implementation Schedule:\n   - Phase 1: Core components (LLMClient, NoteGenerator) - 3 days\n   - Phase 2: MasterOrchestrator and account-specific configs - 2 days\n   - Phase 3: CLI and supporting modules - 2 days\n   - Phase 4: Cleanup and removal of V3 settings - 1 day\n\n5. Backward Compatibility Strategy:\n   - Maintain dual-path for 1 release cycle\n   - Add deprecation warnings when V3 settings are accessed\n   - Document migration path for users with custom plugins\n</info added on 2026-01-17T20:03:54.065Z>\n\n<info added on 2026-01-17T20:05:53.122Z>\n<info added on 2026-01-18T15:42:11.023Z>\n## Migration Progress Update and Technical Implementation Details\n\n### Current Migration Status\n- **Migration Progress**: ~60% complete (5/8 core modules migrated)\n- **Remaining Effort**: Estimated 3-4 days to complete remaining modules\n\n### Technical Implementation Details for Remaining Modules\n\n1. **Summarization Modules Migration**:\n   ```python\n   # Current pattern in summarization.py\n   max_tokens = settings.get(\"MAX_SUMMARY_TOKENS\", 150)\n   \n   # New pattern\n   max_tokens = self.config.get(\"summarization.max_tokens\", 150)\n   ```\n   - Key config paths to migrate: `MAX_SUMMARY_TOKENS`, `SUMMARY_MODEL`, `SUMMARY_TEMPERATURE`\n   - Create new config schema section: `summarization: {max_tokens: int, model: str, temperature: float}`\n\n2. **Prompt Renderer Migration**:\n   - Create new `PromptManager` class that accepts config in constructor\n   - Move prompt templates from settings to config files\n   - Implement template validation in ConfigLoader\n   - Example implementation:\n   ```python\n   class PromptManager:\n       def __init__(self, config=None):\n           self.config = config or ConfigLoader().load()\n           self.templates = self.config.get(\"prompts.templates\", {})\n           \n       def get_template(self, template_name):\n           return self.templates.get(template_name, \"\")\n   ```\n\n3. **CLI-specific Modules**:\n   - For `cleanup_flags.py` and `backfill.py`, implement hybrid approach:\n     ```python\n     def run_cleanup(config=None):\n         config = config or ConfigLoader().load()\n         # Use config.get() with appropriate paths\n     ```\n   - Add command-line parameter to specify config file location\n\n4. **Configuration Validation**:\n   - Implement JSON schema validation for V4 config format\n   - Add migration warnings when loading legacy config format\n   - Create config upgrade utility:\n   ```python\n   def upgrade_v3_config(v3_config_path, output_path=None):\n       \"\"\"Convert V3 config file to V4 format\"\"\"\n       # Implementation details\n   ```\n\n5. **Testing Strategy for Remaining Modules**:\n   - Create parameterized tests for each config parameter\n   - Test both direct config access and CLI parameter passing\n   - Verify config inheritance works correctly (defaults → file → env vars → CLI args)\n</info added on 2026-01-18T15:42:11.023Z>\n</info added on 2026-01-17T20:05:53.122Z>\n\n<info added on 2026-01-17T20:09:57.149Z>\n## V3-V4 Functionality Gap Analysis and Migration Strategy\n\n### Gap Analysis Summary\nBased on the comprehensive analysis in docs/v3-v4-functionality-gap-analysis.md:\n\n- **Critical Gaps**: 11 major features require implementation in V4 ConfigLoader\n- **Migration Blockers**: 4 high-priority features must be implemented before V3 removal\n- **Technical Debt**: 7 medium/low priority features need roadmap decisions\n\n### Implementation Plan for High-Priority Gaps\n\n1. **UID and Force-Reprocess Flags**:\n   ```python\n   # Implementation in ConfigLoader\n   def load_runtime_flags(self, args=None):\n       \"\"\"Load runtime flags from command line arguments\"\"\"\n       self.config.set(\"runtime.uid\", args.uid if hasattr(args, 'uid') else None)\n       self.config.set(\"runtime.force_reprocess\", args.force_reprocess if hasattr(args, 'force_reprocess') else False)\n   ```\n\n2. **Single-Account Mode**:\n   - Create `AccountManager` class that reads from ConfigLoader\n   - Implement account filtering based on config value\n   - Add CLI parameter with appropriate priority in config cascade\n\n3. **Backfill Command Migration**:\n   - Create dedicated ConfigLoader section for backfill settings\n   - Implement parameter validation specific to backfill operation\n   - Preserve all existing functionality before V3 removal\n\n### Transition Strategy\n\n1. **Dual-Path Implementation**:\n   - Create wrapper functions that check both V3 and V4 configurations\n   - Log deprecation warnings when V3 paths are used\n   - Document migration path for each feature\n\n2. **Feature Verification Process**:\n   - Create verification checklist for each migrated feature\n   - Implement automated tests comparing V3 and V4 behavior\n   - Require sign-off before removing V3 code paths\n\n3. **Documentation Updates**:\n   - Update user documentation with new configuration paths\n   - Create migration guide for users with custom configurations\n   - Document any breaking changes and provide upgrade scripts\n</info added on 2026-01-17T20:09:57.149Z>",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 6,
          "title": "Migrate Logging from v3_logger to V4 Logging System",
          "description": "Replace all instances of v3_logger with the V4 logging system throughout the CLI implementation and remove v3_logger.py.",
          "dependencies": [
            3,
            4
          ],
          "details": "Implementation steps:\n1. Identify all logging calls in cli_v3.py and the new V4 CLI implementation\n2. Replace v3_logger imports with V4 logging imports\n3. Update logging method calls to use V4 logging patterns\n4. Ensure log levels are correctly mapped between V3 and V4\n5. Update any custom logging formatters or handlers\n6. Implement any CLI-specific logging configuration\n7. Update log message text to reflect V4 terminology\n8. Ensure logging initialization happens at the appropriate time in the application lifecycle\n9. Remove v3_logger.py completely\n10. Verify no v3_logger imports remain in the codebase\n\nTesting approach: Verify that all logging events are correctly captured and formatted using the V4 logging system.",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 7,
          "title": "Migrate Error Handling to V4 Mechanisms",
          "description": "Update all error handling in the CLI to use V4 error handling mechanisms and remove error_handling_v3.py.",
          "dependencies": [
            3,
            6
          ],
          "details": "Implementation steps:\n1. Identify all error handling patterns in cli_v3.py\n2. Create mapping of V3 error types to V4 error types\n3. Update exception catching and handling to use V4 patterns\n4. Implement appropriate error reporting using V4 mechanisms\n5. Update error messages to reflect V4 terminology\n6. Ensure consistent error codes and exit statuses\n7. Implement any missing error types needed for CLI functionality\n8. Update error formatting and presentation to users\n9. Remove error_handling_v3.py completely\n10. Verify no error_handling_v3 imports remain in the codebase\n\nTesting approach: Create tests that verify errors are correctly caught, handled, and reported using V4 error handling mechanisms.",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 8,
          "title": "Remove Pipeline Class and Update Orchestrator Integration",
          "description": "Remove the Pipeline class and update the CLI to use MasterOrchestrator exclusively for all command execution.",
          "dependencies": [
            3,
            5,
            7
          ],
          "details": "Implementation steps:\n1. Identify all uses of the Pipeline class in the CLI and throughout the codebase\n2. Refactor code to use MasterOrchestrator instead of Pipeline\n3. Update command execution flow to align with MasterOrchestrator patterns\n4. Remove Pipeline class imports and dependencies\n5. Update any Pipeline-specific configuration to use MasterOrchestrator configuration\n6. Ensure all Pipeline functionality is available through MasterOrchestrator\n7. Update any error handling specific to Pipeline execution\n8. Verify that all commands use MasterOrchestrator consistently\n9. Remove Pipeline class definition completely\n\nTesting approach: Create tests that verify all commands execute correctly through MasterOrchestrator and produce the expected results.",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 9,
          "title": "Remove Backward Compatibility Code",
          "description": "Identify and remove all backward compatibility code, adapters, and shims that were used to support V3 components.",
          "dependencies": [
            4,
            5,
            6,
            7,
            8
          ],
          "details": "Implementation steps:\n1. Identify all backward compatibility code throughout the codebase\n2. Remove V3-specific imports and dependencies\n3. Remove adapter classes and methods that bridge V3 and V4\n4. Remove compatibility flags and version checks\n5. Clean up any deprecated methods or classes\n6. Remove any V3-specific configuration handling\n7. Update docstrings and comments to remove V3 references\n8. Remove any conditional code that checks for V3 vs V4\n9. Verify that no V3 components are referenced anywhere in the codebase\n10. Perform a final sweep to ensure all backward compatibility layers are removed\n\nTesting approach: Use static analysis tools to verify that no V3 components are imported or referenced, and run the full test suite to ensure functionality is preserved.",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 10,
          "title": "Integrate V4 UI/Output Formatting with DryRunOutput",
          "description": "Update the CLI to use V4 output formatting mechanisms including DryRunOutput for consistent user experience.",
          "dependencies": [
            3,
            7
          ],
          "details": "Implementation steps:\n1. Identify all output formatting in cli_v3.py\n2. Update CLI to use V4 output formatting classes and methods\n3. Implement DryRunOutput integration for all commands that support dry run mode\n4. Standardize output formats across all commands\n5. Update progress reporting to use V4 mechanisms\n6. Implement any missing output formatters needed for CLI functionality\n7. Ensure consistent styling and formatting across all command outputs\n8. Update any command-specific output handling\n9. Remove any V3-specific output formatting code\n\nTesting approach: Create tests that verify output formatting is consistent across all commands and matches expected V4 patterns.",
          "status": "done",
          "parentTaskId": 22
        },
        {
          "id": 11,
          "title": "Update Tests for V4 CLI Implementation",
          "description": "Update existing tests and create new tests to verify the functionality of the V4 CLI implementation, removing all V3 test code.",
          "dependencies": [
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10
          ],
          "details": "Implementation steps:\n1. Identify all tests related to cli_v3.py and V3 components\n2. Remove tests that are specific to V3 functionality or backward compatibility\n3. Update test imports to use V4 CLI components only\n4. Update test assertions to match V4 output patterns\n5. Create new tests for V4-specific functionality\n6. Update mock objects and test fixtures to use V4 components\n7. Implement integration tests that verify end-to-end functionality\n8. Update any test utilities or helpers to support V4 testing\n9. Ensure test coverage is maintained or improved for all CLI functionality\n10. Verify that no tests reference V3 modules or components\n\nTesting approach: Run the full test suite and verify that all tests pass with the V4 CLI implementation.",
          "status": "pending",
          "parentTaskId": 22
        },
        {
          "id": 12,
          "title": "Remove V3 Modules and Verify Clean Codebase",
          "description": "Delete all V3-specific modules and verify that no V3 code remains in the codebase.",
          "dependencies": [
            5,
            6,
            7,
            9
          ],
          "details": "Implementation steps:\n1. Delete config_v3_loader.py\n2. Delete config_v3_schema.py\n3. Delete settings.py (V3 settings facade)\n4. Delete v3_logger.py\n5. Delete error_handling_v3.py\n6. Delete cli_v3.py\n7. Perform a comprehensive search for any remaining V3 imports or references\n8. Remove any V3-specific comments or documentation\n9. Verify that all imports have been updated to use V4 components\n10. Confirm that no V3 modules are referenced in configuration files or setup scripts\n\nTesting approach: Use file system and code search tools to verify that all V3 modules have been deleted and no V3 references remain.",
          "status": "pending",
          "parentTaskId": 22
        },
        {
          "id": 13,
          "title": "Update Documentation and Perform Final Verification",
          "description": "Update all documentation to reflect the V4-only architecture and perform final verification of the migration.",
          "dependencies": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12
          ],
          "details": "Implementation steps:\n1. Update user-facing documentation to reflect V4 CLI commands and options\n2. Update developer documentation to reflect V4 architecture\n3. Update README files and getting started guides\n4. Update any command help text or usage examples\n5. Remove any migration guides or V3 references from documentation\n6. Update API documentation for CLI components\n7. Update any architecture diagrams or design documentation\n8. Perform final verification against the migration plan created in subtask 1\n9. Conduct user acceptance testing with stakeholders\n10. Verify that documentation no longer references V3 or backward compatibility\n\nTesting approach: Verify that all documentation accurately reflects the V4-only CLI implementation and conduct a final review of all functionality against the original requirements.",
          "status": "pending",
          "parentTaskId": 22
        }
      ]
    }
  ],
  "metadata": {
    "projectName": "Email Agent V4 'Orchestrator' Upgrade",
    "totalTasks": 20,
    "sourceFile": "c:\\Users\\Marc Bielert\\Github\\email-agent\\scripts\\prd-v4.txt",
    "generatedAt": "2023-11-09"
  }
}