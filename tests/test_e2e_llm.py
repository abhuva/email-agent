"""
End-to-end tests with live LLM API connections.

These tests verify the complete V3 LLM client functionality using real LLM API calls.
They require valid LLM API credentials and should be skipped if credentials are not available.

To run these tests:
    pytest tests/test_e2e_llm.py -v

To skip these tests (if credentials not available):
    pytest tests/test_e2e_llm.py -v -m "not e2e_llm"

Requirements:
    - Valid OpenRouter configuration in config/config.yaml
    - OPENROUTER_API_KEY environment variable in .env
    - Valid API key with sufficient credits/quota
    - Network access to OpenRouter API

Note: These tests will make actual API calls and consume API credits.
Use a test/staging API key if possible, not production.
"""

import pytest
import os
import sys
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from unittest.mock import patch, MagicMock

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.settings import Settings
from src.llm_client import LLMClient, LLMResponse, LLMAPIError, LLMResponseParseError
from src.config import ConfigError

logger = logging.getLogger(__name__)

# Pytest marker for E2E LLM tests
pytestmark = pytest.mark.e2e_llm


# ============================================================================
# Fixtures and Helpers
# ============================================================================

def has_llm_credentials() -> bool:
    """Check if LLM API credentials are available for testing."""
    try:
        config_path = project_root / 'config' / 'config.yaml'
        env_path = project_root / '.env'
        
        if not config_path.exists() or not env_path.exists():
            return False
        
        # Try to load settings and check for API key
        try:
            settings = Settings()
            settings.initialize(str(config_path), str(env_path))
            api_key = settings.get_openrouter_api_key()
            api_url = settings.get_openrouter_api_url()
            model = settings.get_openrouter_model()
            # Reset settings instance
            Settings._instance = None
            Settings._config = None
            return bool(api_key and api_url and model)
        except (ConfigError, Exception) as e:
            logger.debug(f"Failed to load LLM credentials: {e}")
            return False
    except Exception as e:
        logger.debug(f"Error checking LLM credentials: {e}")
        return False


@pytest.fixture(scope="module")
def live_llm_config():
    """Load live LLM configuration for E2E tests."""
    if not has_llm_credentials():
        pytest.skip("LLM API credentials not available - skipping live E2E tests")
    
    config_path = project_root / 'config' / 'config.yaml'
    env_path = project_root / '.env'
    
    # Initialize settings
    settings = Settings()
    settings.initialize(str(config_path), str(env_path))
    
    yield {
        'api_key': settings.get_openrouter_api_key(),
        'api_url': settings.get_openrouter_api_url(),
        'model': settings.get_openrouter_model(),
        'temperature': settings.get_openrouter_temperature(),
        'retry_attempts': settings.get_openrouter_retry_attempts(),
        'retry_delay_seconds': settings.get_openrouter_retry_delay_seconds()
    }
    
    # Cleanup
    Settings._instance = None
    Settings._config = None


@pytest.fixture
def live_llm_client(live_llm_config):
    """Create a live LLM client for testing."""
    client = LLMClient()
    return client


@pytest.fixture
def sample_email_content():
    """Sample email content for testing."""
    return """Subject: Test Email

From: sender@example.com
To: recipient@example.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

This is a test email for LLM classification.
It contains some content that should be analyzed.
"""


@pytest.fixture
def important_email_content():
    """Sample important email content."""
    return """Subject: Urgent: Action Required

From: manager@company.com
To: team@company.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

This is an urgent email requiring immediate attention.
Please review and respond as soon as possible.
"""


@pytest.fixture
def spam_email_content():
    """Sample spam email content."""
    return """Subject: WINNER!!! Claim Your Prize Now!!!

From: winner@spam.com
To: you@example.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

Congratulations! You have won $1,000,000!
Click here now to claim your prize: http://spam.com/claim
Limited time offer! Act now!
"""


@pytest.fixture
def long_email_content():
    """Sample long email content (for truncation testing)."""
    return "Subject: Long Email\n\n" + "This is a very long email. " * 1000


# ============================================================================
# Test Classes
# ============================================================================

class TestE2ELLMConnection:
    """Test LLM client initialization and configuration."""
    
    def test_llm_client_initializes(self, live_llm_config):
        """Test that LLM client can be initialized."""
        client = LLMClient()
        assert client is not None
    
    def test_llm_client_loads_config(self, live_llm_client, live_llm_config):
        """Test that LLM client loads configuration correctly."""
        # Configuration is loaded lazily, so we need to trigger it
        # by making a call or accessing internal state
        # For now, just verify client exists
        assert live_llm_client is not None


class TestE2ELLMClassification:
    """Test LLM email classification with live API."""
    
    def test_classify_simple_email(self, live_llm_client, sample_email_content):
        """Test classifying a simple email."""
        response = live_llm_client.classify_email(sample_email_content)
        
        assert isinstance(response, LLMResponse)
        assert isinstance(response.spam_score, int)
        assert isinstance(response.importance_score, int)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
        assert response.raw_response is not None
    
    def test_classify_important_email(self, live_llm_client, important_email_content):
        """Test classifying an important email."""
        response = live_llm_client.classify_email(important_email_content)
        
        assert isinstance(response, LLMResponse)
        # Important emails should have higher importance scores
        # (but we can't guarantee exact values, so just check range)
        assert 0 <= response.importance_score <= 10
        assert 0 <= response.spam_score <= 10
    
    def test_classify_spam_email(self, live_llm_client, spam_email_content):
        """Test classifying a spam email."""
        response = live_llm_client.classify_email(spam_email_content)
        
        assert isinstance(response, LLMResponse)
        # Spam emails should have higher spam scores
        # (but we can't guarantee exact values, so just check range)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_classify_with_custom_prompt(self, live_llm_client, sample_email_content):
        """Test classification with a custom user prompt."""
        custom_prompt = "Analyze this email and determine if it's important."
        response = live_llm_client.classify_email(
            sample_email_content,
            user_prompt=custom_prompt
        )
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_classify_with_max_chars(self, live_llm_client, long_email_content):
        """Test that email content is truncated when exceeding max_chars."""
        max_chars = 500
        response = live_llm_client.classify_email(
            long_email_content,
            max_chars=max_chars
        )
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
        # Verify truncation occurred (content passed to API should be limited)
        # Note: We can't directly verify the truncated content, but if it works,
        # the API call succeeded with truncated content


class TestE2ELLMResponseParsing:
    """Test LLM response parsing and validation."""
    
    def test_response_has_valid_scores(self, live_llm_client, sample_email_content):
        """Test that response contains valid score ranges."""
        response = live_llm_client.classify_email(sample_email_content)
        
        # Scores should be in valid range
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_response_has_raw_content(self, live_llm_client, sample_email_content):
        """Test that response includes raw API response."""
        response = live_llm_client.classify_email(sample_email_content)
        
        assert response.raw_response is not None
        assert isinstance(response.raw_response, str)
        assert len(response.raw_response) > 0
    
    def test_response_to_dict(self, live_llm_client, sample_email_content):
        """Test converting response to dictionary."""
        response = live_llm_client.classify_email(sample_email_content)
        
        response_dict = response.to_dict()
        assert isinstance(response_dict, dict)
        assert 'spam_score' in response_dict
        assert 'importance_score' in response_dict
        assert response_dict['spam_score'] == response.spam_score
        assert response_dict['importance_score'] == response.importance_score


class TestE2ELLMErrorHandling:
    """Test LLM error handling and recovery."""
    
    def test_invalid_api_key_handling(self, live_llm_config):
        """Test handling of invalid API key."""
        # Create client with invalid key
        # Need to patch where LLMClient imports settings from
        with patch('src.llm_client.settings') as mock_settings:
            mock_settings.get_openrouter_api_key.return_value = 'invalid_key'
            mock_settings.get_openrouter_api_url.return_value = live_llm_config['api_url']
            mock_settings.get_openrouter_model.return_value = live_llm_config['model']
            mock_settings.get_openrouter_temperature.return_value = live_llm_config['temperature']
            mock_settings.get_openrouter_retry_attempts.return_value = 1  # Fast failure
            mock_settings.get_openrouter_retry_delay_seconds.return_value = 1
            mock_settings.get_max_body_chars.return_value = 4000
            
            # Create new client instance after patching
            client = LLMClient()
            # Reset any cached config
            client._api_key = None
            client._api_url = None
            client._model = None
            client._temperature = None
            client._retry_attempts = None
            client._retry_delay_seconds = None
            
            with pytest.raises(LLMAPIError):
                client.classify_email("Test email content")
    
    def test_network_error_handling(self, live_llm_config):
        """Test handling of network errors."""
        # Create client with invalid URL
        # Need to patch where LLMClient imports settings from
        with patch('src.llm_client.settings') as mock_settings:
            mock_settings.get_openrouter_api_key.return_value = live_llm_config['api_key']
            mock_settings.get_openrouter_api_url.return_value = 'https://invalid-url-that-does-not-exist-12345.com/api/v1'
            mock_settings.get_openrouter_model.return_value = live_llm_config['model']
            mock_settings.get_openrouter_temperature.return_value = live_llm_config['temperature']
            mock_settings.get_openrouter_retry_attempts.return_value = 1  # Fast failure
            mock_settings.get_openrouter_retry_delay_seconds.return_value = 1
            mock_settings.get_max_body_chars.return_value = 4000
            
            # Create new client instance after patching
            client = LLMClient()
            # Reset any cached config
            client._api_key = None
            client._api_url = None
            client._model = None
            client._temperature = None
            client._retry_attempts = None
            client._retry_delay_seconds = None
            
            with pytest.raises(LLMAPIError):
                client.classify_email("Test email content")
    
    def test_retry_logic_on_transient_error(self, live_llm_client, sample_email_content):
        """Test that retry logic works on transient errors."""
        # This test is difficult to implement without mocking the API
        # For now, we just verify that the client has retry logic configured
        # Actual retry behavior is tested in unit tests
        
        # Just verify the client can make a successful call
        # (retry logic is tested in unit tests with mocks)
        response = live_llm_client.classify_email(sample_email_content)
        assert isinstance(response, LLMResponse)


class TestE2ELLMPromptConstruction:
    """Test LLM prompt construction."""
    
    def test_default_prompt_format(self, live_llm_client, sample_email_content):
        """Test that default prompt is correctly formatted."""
        response = live_llm_client.classify_email(sample_email_content)
        
        # If we get a valid response, the prompt was correctly formatted
        assert isinstance(response, LLMResponse)
        assert response.raw_response is not None
    
    def test_custom_prompt_integration(self, live_llm_client, sample_email_content):
        """Test that custom prompts are correctly integrated."""
        custom_prompt = "This is a custom prompt for testing."
        response = live_llm_client.classify_email(
            sample_email_content,
            user_prompt=custom_prompt
        )
        
        # If we get a valid response, the custom prompt was correctly integrated
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10


class TestE2ELLMEmailTypes:
    """Test LLM classification with various email types."""
    
    def test_plain_text_email(self, live_llm_client):
        """Test classification of plain text email."""
        email_content = """Subject: Plain Text Email

This is a simple plain text email without any special formatting.
It should be classified normally.
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_html_email(self, live_llm_client):
        """Test classification of HTML email."""
        email_content = """Subject: HTML Email

<html>
<body>
<h1>This is an HTML email</h1>
<p>It contains HTML formatting.</p>
</body>
</html>
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_email_with_special_characters(self, live_llm_client):
        """Test classification of email with special characters."""
        email_content = """Subject: Email with Special Characters

This email contains special characters: !@#$%^&*()_+-=[]{}|;':\",./<>?
It also has unicode: ‰Ω†Â•Ω‰∏ñÁïå üåç
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_empty_email(self, live_llm_client):
        """Test classification of empty email."""
        email_content = ""
        
        # Empty email should still work (though may have edge cases)
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10


class TestE2ELLMIntegration:
    """Test LLM client integration with full pipeline."""
    
    def test_llm_client_with_decision_logic(self, live_llm_client, sample_email_content):
        """Test LLM client integration with decision logic."""
        from src.decision_logic import DecisionLogic, ClassificationResult
        
        # Get LLM response
        llm_response = live_llm_client.classify_email(sample_email_content)
        
        # Use decision logic to classify
        decision_logic = DecisionLogic()
        classification = decision_logic.classify(llm_response)
        
        assert isinstance(classification, ClassificationResult)
        assert classification.importance_score == llm_response.importance_score
        assert classification.spam_score == llm_response.spam_score
    
    def test_llm_client_with_orchestrator(self, live_llm_client, sample_email_content):
        """Test LLM client integration with orchestrator (dry-run)."""
        from src.orchestrator import Pipeline, ProcessOptions
        from src.dry_run import set_dry_run, is_dry_run
        
        # Enable dry-run to avoid actual file operations
        set_dry_run(True)
        
        try:
            # This test requires IMAP connection, so we'll skip if not available
            # For now, just verify LLM client works independently
            response = live_llm_client.classify_email(sample_email_content)
            assert isinstance(response, LLMResponse)
        finally:
            set_dry_run(False)


class TestE2ELLMPerformance:
    """Test LLM API performance characteristics."""
    
    def test_classification_response_time(self, live_llm_client, sample_email_content):
        """Test that classification completes in reasonable time."""
        import time
        
        start_time = time.time()
        response = live_llm_client.classify_email(sample_email_content)
        end_time = time.time()
        
        elapsed_time = end_time - start_time
        
        # Should complete within 60 seconds (API timeout)
        assert elapsed_time < 60
        assert isinstance(response, LLMResponse)
        
        logger.info(f"LLM classification took {elapsed_time:.2f} seconds")
    
    def test_multiple_classifications(self, live_llm_client, sample_email_content):
        """Test multiple sequential classifications."""
        responses = []
        
        for i in range(3):
            response = live_llm_client.classify_email(sample_email_content)
            responses.append(response)
        
        assert len(responses) == 3
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10


# ============================================================================
# E2E Tests: Edge Cases
# ============================================================================

class TestE2EEdgeCases:
    """Test edge cases with live LLM API."""
    
    def test_very_large_email_classification(self, live_llm_client):
        """Test classification of very large emails (edge case)."""
        # Create a very large email (exceeding typical limits)
        large_email = "Subject: Very Large Email\n\n"
        large_email += "This is a very large email. " * 5000  # ~150KB
        
        # Should handle large emails with truncation
        response = live_llm_client.classify_email(large_email)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
        
        # Verify truncation occurred (content should be limited)
        # The API should receive truncated content, not the full email
        logger.info(f"Large email classification succeeded (truncated)")
    
    def test_rate_limiting_scenario(self, live_llm_client, sample_email_content):
        """Test behavior under rate limiting scenarios (edge case)."""
        import time
        
        # Make multiple rapid requests to test rate limiting
        responses = []
        errors = []
        request_count = 0
        max_requests = 10  # Reasonable limit for testing
        
        for i in range(max_requests):
            try:
                response = live_llm_client.classify_email(sample_email_content)
                responses.append(response)
                request_count += 1
                
                # Add small delay to avoid hitting rate limits too aggressively
                time.sleep(0.5)
                
            except LLMAPIError as e:
                errors.append(e)
                # Rate limiting may cause errors
                logger.warning(f"Rate limit hit at request {request_count}: {e}")
                # Should handle gracefully
                break
            except Exception as e:
                errors.append(e)
                logger.warning(f"Unexpected error at request {request_count}: {e}")
                break
        
        # Should have made at least some successful requests
        assert request_count > 0 or len(errors) > 0
        
        # Verify successful responses are valid
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
    
    def test_connection_interruption_recovery(self, live_llm_client, sample_email_content):
        """Test recovery from connection interruptions (edge case)."""
        # Simulate connection interruption by making request with network issues
        # This is difficult to test with live API, so we test retry logic
        
        # Make a normal request first
        response1 = live_llm_client.classify_email(sample_email_content)
        assert isinstance(response1, LLMResponse)
        
        # Make another request (simulating reconnection)
        response2 = live_llm_client.classify_email(sample_email_content)
        assert isinstance(response2, LLMResponse)
        
        # Both should succeed (client should handle reconnection)
        assert 0 <= response1.spam_score <= 10
        assert 0 <= response2.spam_score <= 10
    
    def test_malformed_response_handling(self, live_llm_client):
        """Test handling of malformed API responses (edge case)."""
        # This is difficult to test with live API since we can't control responses
        # But we can test with edge case inputs that might cause issues
        
        # Test with empty email
        try:
            response = live_llm_client.classify_email("")
            # Should either succeed or raise appropriate error
            if response:
                assert isinstance(response, LLMResponse)
        except LLMResponseParseError:
            # Malformed response should raise parse error
            pass
        except LLMAPIError:
            # API error is also acceptable
            pass
    
    def test_malformed_json_response(self, live_llm_client, sample_email_content):
        """Test handling when API returns malformed JSON (edge case)."""
        # This is difficult to test with live API, but we can verify
        # that the client has error handling for malformed responses
        
        # Make a normal request - if API returns malformed JSON,
        # it should be caught by response parsing
        try:
            response = live_llm_client.classify_email(sample_email_content)
            # If successful, verify response is valid
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
        except LLMResponseParseError:
            # Malformed JSON should raise parse error
            # This is acceptable behavior
            pass
    
    def test_timeout_scenario(self, live_llm_client, sample_email_content):
        """Test handling of API timeout scenarios (edge case)."""
        import time
        
        # Make request and measure time
        start_time = time.time()
        
        try:
            response = live_llm_client.classify_email(sample_email_content)
            elapsed_time = time.time() - start_time
            
            # Should complete within reasonable time (60 seconds)
            assert elapsed_time < 60
            assert isinstance(response, LLMResponse)
            
            logger.info(f"Request completed in {elapsed_time:.2f} seconds")
            
        except LLMAPIError as e:
            # Timeout errors should be handled gracefully
            elapsed_time = time.time() - start_time
            logger.warning(f"Request timed out after {elapsed_time:.2f} seconds: {e}")
            # Should raise appropriate error, not crash
            assert "timeout" in str(e).lower() or "time" in str(e).lower() or True
    
    def test_concurrent_classifications(self, live_llm_client, sample_email_content):
        """Test concurrent LLM API calls (edge case)."""
        import threading
        import time
        
        responses = []
        errors = []
        
        def classify_email():
            try:
                response = live_llm_client.classify_email(sample_email_content)
                responses.append(response)
            except Exception as e:
                errors.append(e)
        
        # Start multiple threads
        threads = []
        for i in range(3):
            thread = threading.Thread(target=classify_email)
            threads.append(thread)
            thread.start()
        
        # Wait for all threads
        for thread in threads:
            thread.join(timeout=60)
        
        # Should have some results or errors (both are acceptable)
        # Key is that it doesn't crash
        assert len(responses) + len(errors) > 0
        
        # Verify successful responses are valid
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
    
    def test_extremely_long_email(self, live_llm_client):
        """Test classification of extremely long emails (edge case)."""
        # Create an extremely long email
        extremely_long_email = "Subject: Extremely Long Email\n\n"
        extremely_long_email += "This is an extremely long email. " * 10000  # ~300KB
        
        # Should handle with truncation
        try:
            response = live_llm_client.classify_email(extremely_long_email)
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
        except LLMAPIError as e:
            # Some APIs may reject extremely long content
            # This is acceptable behavior
            logger.info(f"Extremely long email rejected: {e}")
            assert "too long" in str(e).lower() or "limit" in str(e).lower() or True
    
    def test_special_characters_and_unicode(self, live_llm_client):
        """Test classification with special characters and unicode (edge case)."""
        # Test with various special characters and unicode
        test_cases = [
            "Subject: Test\n\nEmail with emojis: üòÄ üéâ üöÄ",
            "Subject: Test\n\nEmail with unicode: ‰Ω†Â•Ω‰∏ñÁïå ŸÖÿ±ÿ≠ÿ®ÿß",
            "Subject: Test\n\nEmail with special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?",
            "Subject: Test\n\nEmail with newlines:\n\nLine 1\nLine 2\nLine 3",
            "Subject: Test\n\nEmail with tabs:\tTabbed\tcontent",
        ]
        
        for email_content in test_cases:
            try:
                response = live_llm_client.classify_email(email_content)
                assert isinstance(response, LLMResponse)
                assert 0 <= response.spam_score <= 10
                assert 0 <= response.importance_score <= 10
            except Exception as e:
                # Some edge cases may fail, but should fail gracefully
                logger.warning(f"Special character test failed: {e}")
                assert isinstance(e, (LLMAPIError, LLMResponseParseError))
    
    def test_empty_and_minimal_emails(self, live_llm_client):
        """Test classification of empty and minimal emails (edge case)."""
        test_cases = [
            "",  # Empty
            "Subject:",  # Minimal
            "Subject: Test",  # Subject only
            "Test",  # No headers
        ]
        
        for email_content in test_cases:
            try:
                response = live_llm_client.classify_email(email_content)
                # Should either succeed or raise appropriate error
                if response:
                    assert isinstance(response, LLMResponse)
                    assert 0 <= response.spam_score <= 10
                    assert 0 <= response.importance_score <= 10
            except (LLMAPIError, LLMResponseParseError) as e:
                # Empty/minimal emails may cause errors, which is acceptable
                logger.info(f"Empty/minimal email test: {e}")
                pass
    
    def test_rapid_successive_requests(self, live_llm_client, sample_email_content):
        """Test making rapid successive API requests (edge case)."""
        import time
        
        responses = []
        start_time = time.time()
        
        # Make 5 rapid requests
        for i in range(5):
            try:
                response = live_llm_client.classify_email(sample_email_content)
                responses.append(response)
                # Small delay to avoid overwhelming API
                time.sleep(0.2)
            except LLMAPIError as e:
                # Rate limiting may occur
                logger.warning(f"Rapid request {i} failed: {e}")
                break
        
        elapsed_time = time.time() - start_time
        
        # Should have made at least some successful requests
        assert len(responses) > 0 or elapsed_time < 60
        
        # Verify successful responses
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
