"""
End-to-end tests with live LLM API connections.

These tests verify the complete V3 LLM client functionality using real LLM API calls.
They require valid LLM API credentials and should be skipped if credentials are not available.

To run these tests:
    pytest tests/test_e2e_llm.py -v

To skip these tests (if credentials not available):
    pytest tests/test_e2e_llm.py -v -m "not e2e_llm"

Requirements:
    - Valid OpenRouter configuration in config/config.yaml
    - OPENROUTER_API_KEY environment variable in .env
    - Valid API key with sufficient credits/quota
    - Network access to OpenRouter API

Note: These tests will make actual API calls and consume API credits.
Use a test/staging API key if possible, not production.
"""

import pytest
import os
import sys
import logging
from pathlib import Path
from typing import Optional, Dict, Any
from unittest.mock import patch, MagicMock

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.settings import Settings
from src.llm_client import LLMClient, LLMResponse, LLMAPIError, LLMResponseParseError
from src.config import ConfigError

logger = logging.getLogger(__name__)

# Pytest marker for E2E LLM tests
pytestmark = pytest.mark.e2e_llm


# ============================================================================
# Fixtures and Helpers
# ============================================================================

def has_llm_credentials() -> bool:
    """Check if LLM API credentials are available for testing."""
    try:
        config_path = project_root / 'config' / 'config.yaml'
        env_path = project_root / '.env'
        
        if not config_path.exists() or not env_path.exists():
            return False
        
        # Try to load settings and check for API key
        try:
            settings = Settings()
            settings.initialize(str(config_path), str(env_path))
            api_key = settings.get_openrouter_api_key()
            api_url = settings.get_openrouter_api_url()
            model = settings.get_openrouter_model()
            # Reset settings instance
            Settings._instance = None
            Settings._config = None
            return bool(api_key and api_url and model)
        except (ConfigError, Exception) as e:
            logger.debug(f"Failed to load LLM credentials: {e}")
            return False
    except Exception as e:
        logger.debug(f"Error checking LLM credentials: {e}")
        return False


@pytest.fixture(scope="module")
def live_llm_config():
    """Load live LLM configuration for E2E tests."""
    if not has_llm_credentials():
        pytest.skip("LLM API credentials not available - skipping live E2E tests")
    
    config_path = project_root / 'config' / 'config.yaml'
    env_path = project_root / '.env'
    
    # Initialize settings
    settings = Settings()
    settings.initialize(str(config_path), str(env_path))
    
    yield {
        'api_key': settings.get_openrouter_api_key(),
        'api_url': settings.get_openrouter_api_url(),
        'model': settings.get_openrouter_model(),
        'temperature': settings.get_openrouter_temperature(),
        'retry_attempts': settings.get_openrouter_retry_attempts(),
        'retry_delay_seconds': settings.get_openrouter_retry_delay_seconds()
    }
    
    # Cleanup
    Settings._instance = None
    Settings._config = None


@pytest.fixture
def live_llm_client(live_llm_config):
    """Create a live LLM client for testing."""
    client = LLMClient()
    return client


@pytest.fixture
def sample_email_content():
    """Sample email content for testing."""
    return """Subject: Test Email

From: sender@example.com
To: recipient@example.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

This is a test email for LLM classification.
It contains some content that should be analyzed.
"""


@pytest.fixture
def important_email_content():
    """Sample important email content."""
    return """Subject: Urgent: Action Required

From: manager@company.com
To: team@company.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

This is an urgent email requiring immediate attention.
Please review and respond as soon as possible.
"""


@pytest.fixture
def spam_email_content():
    """Sample spam email content."""
    return """Subject: WINNER!!! Claim Your Prize Now!!!

From: winner@spam.com
To: you@example.com
Date: Mon, 1 Jan 2024 12:00:00 +0000

Congratulations! You have won $1,000,000!
Click here now to claim your prize: http://spam.com/claim
Limited time offer! Act now!
"""


@pytest.fixture
def long_email_content():
    """Sample long email content (for truncation testing)."""
    return "Subject: Long Email\n\n" + "This is a very long email. " * 1000


# ============================================================================
# Test Classes
# ============================================================================

class TestE2ELLMConnection:
    """Test LLM client initialization and configuration."""
    
    def test_llm_client_initializes(self, live_llm_config):
        """Test that LLM client can be initialized."""
        client = LLMClient()
        assert client is not None
    
    def test_llm_client_loads_config(self, live_llm_client, live_llm_config):
        """Test that LLM client loads configuration correctly."""
        # Configuration is loaded lazily, so we need to trigger it
        # by making a call or accessing internal state
        # For now, just verify client exists
        assert live_llm_client is not None


class TestE2ELLMClassification:
    """Test LLM email classification with live API."""
    
    def test_classify_simple_email(self, live_llm_client, sample_email_content):
        """Test classifying a simple email."""
        response = live_llm_client.classify_email(sample_email_content)
        
        assert isinstance(response, LLMResponse)
        assert isinstance(response.spam_score, int)
        assert isinstance(response.importance_score, int)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
        assert response.raw_response is not None
    
    def test_classify_important_email(self, live_llm_client, important_email_content):
        """Test classifying an important email."""
        response = live_llm_client.classify_email(important_email_content)
        
        assert isinstance(response, LLMResponse)
        # Important emails should have higher importance scores
        # (but we can't guarantee exact values, so just check range)
        assert 0 <= response.importance_score <= 10
        assert 0 <= response.spam_score <= 10
    
    def test_classify_spam_email(self, live_llm_client, spam_email_content):
        """Test classifying a spam email."""
        response = live_llm_client.classify_email(spam_email_content)
        
        assert isinstance(response, LLMResponse)
        # Spam emails should have higher spam scores
        # (but we can't guarantee exact values, so just check range)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_classify_with_custom_prompt(self, live_llm_client, sample_email_content):
        """Test classification with a custom user prompt."""
        custom_prompt = "Analyze this email and determine if it's important."
        response = live_llm_client.classify_email(
            sample_email_content,
            user_prompt=custom_prompt
        )
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_classify_with_max_chars(self, live_llm_client, long_email_content):
        """Test that email content is truncated when exceeding max_chars."""
        max_chars = 500
        response = live_llm_client.classify_email(
            long_email_content,
            max_chars=max_chars
        )
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
        # Verify truncation occurred (content passed to API should be limited)
        # Note: We can't directly verify the truncated content, but if it works,
        # the API call succeeded with truncated content


class TestE2ELLMResponseParsing:
    """Test LLM response parsing and validation."""
    
    def test_response_has_valid_scores(self, live_llm_client, sample_email_content):
        """Test that response contains valid score ranges."""
        response = live_llm_client.classify_email(sample_email_content)
        
        # Scores should be in valid range
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_response_has_raw_content(self, live_llm_client, sample_email_content):
        """Test that response includes raw API response."""
        response = live_llm_client.classify_email(sample_email_content)
        
        assert response.raw_response is not None
        assert isinstance(response.raw_response, str)
        assert len(response.raw_response) > 0
    
    def test_response_to_dict(self, live_llm_client, sample_email_content):
        """Test converting response to dictionary."""
        response = live_llm_client.classify_email(sample_email_content)
        
        response_dict = response.to_dict()
        assert isinstance(response_dict, dict)
        assert 'spam_score' in response_dict
        assert 'importance_score' in response_dict
        assert response_dict['spam_score'] == response.spam_score
        assert response_dict['importance_score'] == response.importance_score


class TestE2ELLMErrorHandling:
    """Test LLM error handling and recovery."""
    
    def test_invalid_api_key_handling(self, live_llm_config):
        """Test handling of invalid API key."""
        # Create client with invalid key
        with patch('src.settings.settings') as mock_settings:
            mock_settings.get_openrouter_api_key.return_value = 'invalid_key'
            mock_settings.get_openrouter_api_url.return_value = live_llm_config['api_url']
            mock_settings.get_openrouter_model.return_value = live_llm_config['model']
            mock_settings.get_openrouter_temperature.return_value = live_llm_config['temperature']
            mock_settings.get_openrouter_retry_attempts.return_value = 1  # Fast failure
            mock_settings.get_openrouter_retry_delay_seconds.return_value = 1
            
            client = LLMClient()
            
            with pytest.raises(LLMAPIError):
                client.classify_email("Test email content")
    
    def test_network_error_handling(self, live_llm_config):
        """Test handling of network errors."""
        # Create client with invalid URL
        with patch('src.settings.settings') as mock_settings:
            mock_settings.get_openrouter_api_key.return_value = live_llm_config['api_key']
            mock_settings.get_openrouter_api_url.return_value = 'https://invalid-url-that-does-not-exist.com/api/v1'
            mock_settings.get_openrouter_model.return_value = live_llm_config['model']
            mock_settings.get_openrouter_temperature.return_value = live_llm_config['temperature']
            mock_settings.get_openrouter_retry_attempts.return_value = 1  # Fast failure
            mock_settings.get_openrouter_retry_delay_seconds.return_value = 1
            
            client = LLMClient()
            
            with pytest.raises(LLMAPIError):
                client.classify_email("Test email content")
    
    def test_retry_logic_on_transient_error(self, live_llm_client, sample_email_content):
        """Test that retry logic works on transient errors."""
        # This test is difficult to implement without mocking the API
        # For now, we just verify that the client has retry logic configured
        # Actual retry behavior is tested in unit tests
        
        # Just verify the client can make a successful call
        # (retry logic is tested in unit tests with mocks)
        response = live_llm_client.classify_email(sample_email_content)
        assert isinstance(response, LLMResponse)


class TestE2ELLMPromptConstruction:
    """Test LLM prompt construction."""
    
    def test_default_prompt_format(self, live_llm_client, sample_email_content):
        """Test that default prompt is correctly formatted."""
        response = live_llm_client.classify_email(sample_email_content)
        
        # If we get a valid response, the prompt was correctly formatted
        assert isinstance(response, LLMResponse)
        assert response.raw_response is not None
    
    def test_custom_prompt_integration(self, live_llm_client, sample_email_content):
        """Test that custom prompts are correctly integrated."""
        custom_prompt = "This is a custom prompt for testing."
        response = live_llm_client.classify_email(
            sample_email_content,
            user_prompt=custom_prompt
        )
        
        # If we get a valid response, the custom prompt was correctly integrated
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10


class TestE2ELLMEmailTypes:
    """Test LLM classification with various email types."""
    
    def test_plain_text_email(self, live_llm_client):
        """Test classification of plain text email."""
        email_content = """Subject: Plain Text Email

This is a simple plain text email without any special formatting.
It should be classified normally.
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_html_email(self, live_llm_client):
        """Test classification of HTML email."""
        email_content = """Subject: HTML Email

<html>
<body>
<h1>This is an HTML email</h1>
<p>It contains HTML formatting.</p>
</body>
</html>
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_email_with_special_characters(self, live_llm_client):
        """Test classification of email with special characters."""
        email_content = """Subject: Email with Special Characters

This email contains special characters: !@#$%^&*()_+-=[]{}|;':\",./<>?
It also has unicode: ‰Ω†Â•Ω‰∏ñÁïå üåç
"""
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10
    
    def test_empty_email(self, live_llm_client):
        """Test classification of empty email."""
        email_content = ""
        
        # Empty email should still work (though may have edge cases)
        response = live_llm_client.classify_email(email_content)
        
        assert isinstance(response, LLMResponse)
        assert 0 <= response.spam_score <= 10
        assert 0 <= response.importance_score <= 10


class TestE2ELLMIntegration:
    """Test LLM client integration with full pipeline."""
    
    def test_llm_client_with_decision_logic(self, live_llm_client, sample_email_content):
        """Test LLM client integration with decision logic."""
        from src.decision_logic import DecisionLogic, ClassificationResult
        
        # Get LLM response
        llm_response = live_llm_client.classify_email(sample_email_content)
        
        # Use decision logic to classify
        decision_logic = DecisionLogic()
        classification = decision_logic.classify(llm_response)
        
        assert isinstance(classification, ClassificationResult)
        assert classification.importance_score == llm_response.importance_score
        assert classification.spam_score == llm_response.spam_score
    
    def test_llm_client_with_orchestrator(self, live_llm_client, sample_email_content):
        """Test LLM client integration with orchestrator (dry-run)."""
        from src.orchestrator import Pipeline, ProcessOptions
        from src.dry_run import set_dry_run, is_dry_run
        
        # Enable dry-run to avoid actual file operations
        set_dry_run(True)
        
        try:
            # This test requires IMAP connection, so we'll skip if not available
            # For now, just verify LLM client works independently
            response = live_llm_client.classify_email(sample_email_content)
            assert isinstance(response, LLMResponse)
        finally:
            set_dry_run(False)


class TestE2ELLMPerformance:
    """Test LLM API performance characteristics."""
    
    def test_classification_response_time(self, live_llm_client, sample_email_content):
        """Test that classification completes in reasonable time."""
        import time
        
        start_time = time.time()
        response = live_llm_client.classify_email(sample_email_content)
        end_time = time.time()
        
        elapsed_time = end_time - start_time
        
        # Should complete within 60 seconds (API timeout)
        assert elapsed_time < 60
        assert isinstance(response, LLMResponse)
        
        logger.info(f"LLM classification took {elapsed_time:.2f} seconds")
    
    def test_multiple_classifications(self, live_llm_client, sample_email_content):
        """Test multiple sequential classifications."""
        responses = []
        
        for i in range(3):
            response = live_llm_client.classify_email(sample_email_content)
            responses.append(response)
        
        assert len(responses) == 3
        for response in responses:
            assert isinstance(response, LLMResponse)
            assert 0 <= response.spam_score <= 10
            assert 0 <= response.importance_score <= 10
